{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Q1)What is unsupervised learning in the context of machine learning ?\n",
        "Ans:Unsupervised learning is a type of machine learning where the model is trained on data that does not have labeled outputs. Unlike supervised learning, where each training example comes with an associated label or outcome, unsupervised learning algorithms explore the inherent structure of the input data without any explicit instruction on what to predict.\n",
        "\n",
        "Key Characteristics:\n",
        "No labeled data: The algorithm is given only input data\n",
        ", and no corresponding target labels\n",
        ".\n",
        "\n",
        "Goal: Discover hidden patterns, groupings, or features in the data.\n",
        "\n",
        "Common tasks:\n",
        "\n",
        "Clustering: Grouping similar data points together (e.g., K-means, DBSCAN).\n",
        "Dimensionality Reduction: Reducing the number of variables or features while preserving important information (e.g., PCA, t-SNE).\n",
        "Anomaly Detection: Identifying unusual data points that do not fit the general pattern.\n",
        "Q2)How does K-Means clustering algorithm work ?\n",
        "Ans: The K-Means clustering algorithm is a popular unsupervised learning technique used to partition a dataset into K distinct, non-overlapping clusters based on similarity.\n",
        "\n",
        "Here's how K-Means works:\n",
        "Step-by-Step Process:\n",
        "Choose the number of clusters (K):\n",
        "\n",
        "Decide how many clusters you want to divide the data into.\n",
        "Initialize K centroids:\n",
        "\n",
        "Randomly select K data points as the initial centroids (cluster centers).\n",
        "Assign data points to the nearest centroid:\n",
        "\n",
        "For each data point, compute the distance (typically Euclidean) to each centroid.\n",
        "Assign the point to the cluster of the closest centroid.\n",
        "Recalculate the centroids:\n",
        "\n",
        "Update the centroid of each cluster by computing the mean of all the points assigned to that cluster.\n",
        "Repeat steps 3 and 4:\n",
        "\n",
        "Continue reassigning points and updating centroids until:\n",
        "\n",
        "Centroids no longer change (convergence), or\n",
        "A maximum number of iterations is reached.\n",
        "Example Illustration:\n",
        "Imagine you want to group customers by spending habits:\n",
        "\n",
        "Input: A set of customer data with features like annual income and spending score.\n",
        "\n",
        "K-Means will:\n",
        "\n",
        "Start with random centroids.\n",
        "Group customers based on proximity to those centroids.\n",
        "Adjust the centroids based on groupings.\n",
        "Repeat until stable groups emerge.\n",
        "Key Concepts:\n",
        "Concept\tDescription\n",
        "Centroid\tThe \"center\" of a cluster.\n",
        "Inertia\tThe sum of squared distances between each point and its centroid (lower is better).\n",
        "Convergence\tWhen assignments no longer change or changes are minimal.\n",
        "K selection\tYou must choose K manually or use techniques like the Elbow Method or Silhouette Score to find an optimal K.\n",
        "Pros & Cons:\n",
        "‚úÖ Pros:\n",
        "\n",
        "Simple and fast.\n",
        "Works well for spherical clusters.\n",
        "‚ùå Cons:\n",
        "\n",
        "Needs you to predefine K.\n",
        "Sensitive to outliers and initial centroid placement.\n",
        "Doesn‚Äôt handle complex shapes or varying cluster densities well.\n",
        "Q3)Explain the concept of a dendrogram in hierarchical clustering ?\n",
        "Ans:A dendrogram is a tree-like diagram used in hierarchical clustering to show how data points are grouped together step by step. Each merge is represented by a branch, and the height of the branch indicates the distance or dissimilarity at which clusters are joined. You can \"cut\" the dendrogram at a chosen height to decide the number of clusters.\n",
        "\n",
        "Q4) What is the main difference between K-Means and Hierarchical Clustering ?\n",
        "Ans:The main difference between K-Means and Hierarchical Clustering is in how they form clusters:\n",
        "\n",
        "K-Means is a partitioning method: It divides the data into K clusters by optimizing cluster centers (centroids). You must specify K in advance, and it updates clusters iteratively.\n",
        "\n",
        "Hierarchical Clustering builds a tree-like structure (dendrogram): It either merges smaller clusters into larger ones (agglomerative) or splits larger clusters into smaller ones (divisive), without needing to predefine the number of clusters.\n",
        "\n",
        "Key Difference:\n",
        "K-Means requires you to set the number of clusters before clustering.\n",
        "Hierarchical Clustering creates a hierarchy and lets you choose the number of clusters after examining the dendrogram.\n",
        "Q5)What are the advantages of DBSCAN over K-Means ?\n",
        "Ans:DBSCAN (Density-Based Spatial Clustering of Applications with Noise) offers several advantages over K-Means, especially for more complex or noisy datasets:\n",
        "\n",
        "‚úÖ Advantages of DBSCAN over K-Means:\n",
        "Feature\tDBSCAN\tK-Means\n",
        "1. No need to specify number of clusters\tDBSCAN automatically detects the number of clusters based on density.\tRequires the number of clusters K to be specified upfront.\n",
        "2. Handles noise/outliers\tEffectively identifies and labels outliers as noise.\tAssigns all points to a cluster, including outliers.\n",
        "3. Detects clusters of arbitrary shapes\tCan find irregular or non-spherical clusters (e.g., curved or elongated).\tAssumes clusters are spherical and evenly sized.\n",
        "4. Robust to varying densities\tCan handle clusters with different densities (with tuning).\tStruggles with clusters of varying sizes or densities.\n",
        "üß† When to Use DBSCAN:\n",
        "When clusters are non-spherical or oddly shaped\n",
        "When the dataset contains noise or outliers\n",
        "When you don‚Äôt know the number of clusters in advance\n",
        "‚ö†Ô∏è Note:\n",
        "DBSCAN has some limitations too ‚Äî it may struggle with high-dimensional data and choosing good parameters (eps and min_samples) can be tricky.\n",
        "\n",
        "Would you like a visual comparison or example code to see the difference?\n",
        "\n",
        "Q6) When would you use Silhouette Score in clustering ?\n",
        "Ans:You would use the Silhouette Score in clustering to evaluate the quality of your clusters ‚Äî specifically, to measure how well each data point fits within its assigned cluster compared to other clusters.\n",
        "\n",
        "‚úÖ Use Cases for Silhouette Score:\n",
        "To choose the optimal number of clusters (K):\n",
        "\n",
        "Run clustering (e.g., K-Means) with different values of K.\n",
        "Calculate the silhouette score for each.\n",
        "Higher scores (closer to 1) indicate better-defined clusters.\n",
        "Helps in selecting the K that gives the best clustering performance.\n",
        "To assess clustering quality:\n",
        "\n",
        "A high average silhouette score indicates that clusters are well-separated and cohesive.\n",
        "A low or negative score suggests that data points may be in the wrong clusters or that clusters are overlapping.\n",
        "To compare different clustering algorithms:\n",
        "\n",
        "Use the silhouette score to determine which algorithm gives the best fit for a particular dataset.\n",
        "üìà Score Range:\n",
        "+1: Data point is well matched to its cluster and poorly matched to others.\n",
        "0: Data point is on or very close to the boundary between clusters.\n",
        "-1: Data point may be assigned to the wrong cluster.\n",
        "üß™ In Short:\n",
        "Use the Silhouette Score to validate and compare clustering results, especially when the true labels are not known.\n",
        "\n",
        "Q7)What are the limitations of Hierarchical Clustering ?\n",
        "Ans:Hierarchical Clustering has several limitations, especially when applied to large or complex datasets:\n",
        "\n",
        "‚ö†Ô∏è Limitations of Hierarchical Clustering:\n",
        "Not scalable to large datasets\n",
        "\n",
        "Time and space complexity is O(n¬≤) or worse.\n",
        "Becomes computationally expensive for large datasets.\n",
        "Irreversible decisions\n",
        "\n",
        "Once two clusters are merged (or split), the decision cannot be undone, even if it later proves suboptimal.\n",
        "Sensitive to noise and outliers\n",
        "\n",
        "Can create misleading merges if outliers are present, affecting the overall clustering structure.\n",
        "Requires distance metric choice\n",
        "\n",
        "Results depend heavily on the distance metric (e.g., Euclidean, Manhattan) and linkage method (e.g., single, complete, average).\n",
        "No clear way to choose number of clusters\n",
        "\n",
        "Unlike K-Means, there‚Äôs no built-in way to select the best number of clusters ‚Äî users must interpret the dendrogram, which can be subjective.\n",
        "Poor performance on high-dimensional data\n",
        "\n",
        "Distance metrics become less meaningful in high-dimensional spaces, reducing the quality of clustering.\n",
        "Q8)Why is feature scaling important in clustering algorithms like K-Means ?\n",
        "Ans:Feature scaling is important in clustering algorithms like K-Means because the algorithm relies on distance calculations (usually Euclidean distance) to assign points to clusters. If features have different scales or units, those with larger values will dominate the distance calculation, causing the clustering to be biased toward those features. Scaling puts all features on a comparable scale, ensuring each feature contributes fairly to the clustering process and resulting in better, more meaningful clusters.\n",
        "\n",
        "Q9)How does DBSCAN identify noise points ?\n",
        "Ans:DBSCAN identifies noise points (also called outliers) based on density criteria:\n",
        "\n",
        "For each point, DBSCAN counts how many points fall within a specified radius eps (neighborhood).\n",
        "If the number of points within this neighborhood is less than a minimum threshold min_samples, the point is classified as noise.\n",
        "Noise points are those that do not belong to any dense region and thus don‚Äôt fit well into any cluster.\n",
        "In short:\n",
        "Noise points are those that have too few neighbors within eps and cannot be assigned to any cluster.\n",
        "\n",
        "Q10)Define inertia in the context of K-Means ?\n",
        "Ans:In the context of K-Means clustering, inertia is a measure of how well the data points fit within their assigned clusters.\n",
        "\n",
        "Specifically, it is the sum of the squared distances between each data point and the centroid of the cluster it belongs to. Formally:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "where:\n",
        "\n",
        " = number of clusters,\n",
        " = points in cluster\n",
        ",\n",
        " = centroid of cluster\n",
        ",\n",
        " = squared Euclidean distance between point\n",
        " and centroid\n",
        ".\n",
        "What does inertia indicate?\n",
        "Lower inertia means data points are closer to their centroids, implying tighter, more compact clusters.\n",
        "K-Means aims to minimize inertia during clustering.\n",
        "Q11)What is the elbow method in K-Means clustering ?\n",
        "Ans:The Elbow Method is a technique used to help choose the optimal number of clusters (K) in K-Means clustering.\n",
        "\n",
        "How it works:\n",
        "Run K-Means clustering on the dataset for a range of K values (e.g., 1 to 10).\n",
        "For each K, calculate the inertia (sum of squared distances of points to their cluster centroids).\n",
        "Plot K (x-axis) vs. inertia (y-axis).\n",
        "Look for an ‚Äúelbow‚Äù point in the plot ‚Äî where the decrease in inertia starts to slow down significantly.\n",
        "Why it matters:\n",
        "Before the elbow, adding clusters greatly reduces inertia (improves clustering).\n",
        "After the elbow, adding more clusters yields diminishing returns (small improvement).\n",
        "The elbow point suggests a good trade-off between cluster complexity and compactness, helping pick a reasonable K.\n",
        "In short:\n",
        "The Elbow Method helps you pick a K where increasing clusters no longer significantly improves clustering quality.\n",
        "\n",
        "Q12)Describe the concept of \"density\" in DBSCAN ?\n",
        "Ans:In DBSCAN, the concept of density refers to how closely data points are packed together in a region of the data space.\n",
        "\n",
        "Key ideas about density in DBSCAN:\n",
        "Density is measured by counting how many points lie within a certain radius (eps) around a given point.\n",
        "A point‚Äôs neighborhood consists of all points within this radius.\n",
        "If the number of points in the neighborhood is at least a minimum threshold (min_samples), the region is considered dense.\n",
        "Points in dense regions are grouped into clusters, while points in sparse regions are labeled as noise.\n",
        "Q13)Can hierarchical clustering be used on categorical data ?\n",
        "Ans:Yes, hierarchical clustering can be used on categorical data, but with some considerations:\n",
        "\n",
        "Since hierarchical clustering relies on a distance (or similarity) measure between data points, you need to use an appropriate distance metric for categorical data (e.g., Hamming distance, Matching coefficient, or other specialized measures).\n",
        "You can also use mixed data distance measures if your data has both categorical and numerical features.\n",
        "Once you define a suitable distance metric, you can apply hierarchical clustering just like with numerical data.\n",
        "In short:\n",
        "Hierarchical clustering can work with categorical data as long as you choose or define a proper distance or similarity measure suitable for categorical variables.\n",
        "\n",
        "Q14)What does a negative Silhouette Score indicate ?\n",
        "Ans:A negative Silhouette Score indicates that a data point is likely assigned to the wrong cluster.\n",
        "\n",
        "What it means:\n",
        "The average distance between the point and other points in its own cluster (cohesion) is greater than the average distance to points in the nearest neighboring cluster (separation).\n",
        "In other words, the point is closer to a different cluster than the one it belongs to, suggesting poor clustering for that point.\n",
        "Summary:\n",
        "A negative silhouette score signals misclassification or overlapping clusters and suggests the clustering solution may need improvement.\n",
        "\n",
        "Q15)Explain the term \"linkage criteria\" in hierarchical clustering ?\n",
        "Ans:Linkage criteria in hierarchical clustering specify how the distance between two clusters is measured when deciding which clusters to merge or split.\n",
        "\n",
        "Common types of linkage:\n",
        "Single linkage: Distance between the closest pair of points in the two clusters.\n",
        "Complete linkage: Distance between the farthest pair of points.\n",
        "Average linkage: Average distance between all pairs of points from the two clusters.\n",
        "Ward‚Äôs linkage: Measures the increase in total within-cluster variance after merging.\n",
        "Why it matters:\n",
        "The linkage method affects the shape and size of clusters and the overall hierarchy produced by the algorithm.\n",
        "\n",
        "In short: Linkage criteria determine how clusters are compared and merged in hierarchical clustering.\n",
        "\n",
        "Q16)Why might K-Means clustering perform poorly on data with varying cluster sizes or densities ?\n",
        "Ans:K-Means clustering performs poorly on data with varying cluster sizes or densities because it assumes clusters are spherical and equally sized. It assigns points to the nearest centroid based on distance, so:\n",
        "\n",
        "Smaller or less dense clusters can be merged into larger, denser clusters.\n",
        "Larger or more spread-out clusters may be split incorrectly.\n",
        "It can‚Äôt handle clusters with different densities well because the distance-based assignment doesn‚Äôt account for density differences.\n",
        "In short, K-Means struggles when clusters aren‚Äôt uniform in size or density, leading to inaccurate clustering.\n",
        "\n",
        "Q17)What are the core parameters in DBSCAN, and how do they influence clustering ?\n",
        "Ans:The core parameters in DBSCAN are:\n",
        "\n",
        "eps (epsilon):\n",
        "\n",
        "Defines the radius of the neighborhood around a point.\n",
        "Determines how close points need to be to each other to be considered neighbors.\n",
        "A smaller eps leads to smaller, tighter clusters, while a larger eps can merge clusters or include more points as neighbors.\n",
        "min_samples:\n",
        "\n",
        "The minimum number of points required to form a dense region (including the point itself).\n",
        "Controls how many points must be within eps to consider a point a core point.\n",
        "Higher values require denser clusters, leading to fewer clusters and more noise points. Lower values can result in more clusters, possibly including noise.\n",
        "How they influence clustering:\n",
        "eps controls the neighborhood size: Too small, and many points become noise; too large, and distinct clusters may merge.\n",
        "min_samples controls cluster density: Higher values demand stricter density, affecting the number and size of clusters and noise.\n",
        "Q18)How does K-Means++ improve upon standard K-Means initialization ?\n",
        "Ans:K-Means++ improves standard K-Means by providing a smarter way to initialize the cluster centroids, which leads to better clustering results and faster convergence.\n",
        "\n",
        "How K-Means++ works:\n",
        "Instead of choosing all initial centroids randomly, K-Means++:\n",
        "\n",
        "Picks the first centroid randomly from the data points.\n",
        "For each remaining centroid, it chooses a new point probabilistically, where points farther from existing centroids have a higher chance of being selected.\n",
        "This spreads out the initial centroids across the data space.\n",
        "Benefits over standard K-Means initialization:\n",
        "Reduces the chance of poor centroid initialization that can cause bad clustering.\n",
        "Leads to faster convergence because initial centroids are well spread out.\n",
        "Often results in lower final inertia (better cluster quality).\n",
        "In short:\n",
        "K-Means++ smartly initializes centroids to improve the stability and quality of K-Means clustering compared to random initialization.\n",
        "\n",
        "Q19)What is agglomerative clustering ?\n",
        "Ans:Agglomerative clustering is a type of hierarchical clustering that builds clusters in a bottom-up manner.\n",
        "\n",
        "How it works:\n",
        "Start: Each data point is its own individual cluster.\n",
        "Merge: At each step, merge the two closest clusters based on a chosen distance metric and linkage criteria.\n",
        "Repeat: Continue merging clusters until all points are grouped into a single cluster or until a stopping criterion (like the desired number of clusters) is met.\n",
        "The process produces a dendrogram showing the hierarchy of clusters.\n",
        "In short:\n",
        "Agglomerative clustering starts with individual points and successively merges them to form larger clusters, creating a tree-like structure of nested clusters.\n",
        "\n",
        "Q20)What makes Silhouette Score a better metric than just inertia for model evaluation?\n",
        "Ans:Silhouette Score is often considered better than just inertia for evaluating clustering because it captures both cohesion and separation, whereas inertia only measures cohesion.\n",
        "\n",
        "Key reasons why Silhouette Score is better:\n",
        "Considers separation between clusters: Silhouette Score evaluates how close each point is to points in its own cluster compared to points in other clusters, reflecting both within-cluster tightness and between-cluster separation. Inertia only measures how close points are to their own cluster centroids, ignoring how distinct clusters are from each other.\n",
        "\n",
        "Normalized and interpretable: Silhouette values range from -1 to +1, making it easier to interpret:\n",
        "\n",
        "Close to +1 means well-clustered points,\n",
        "Around 0 means points near cluster boundaries,\n",
        "Negative values indicate misclassified points. Inertia is an unbounded sum of squared distances, harder to interpret and compare across datasets.\n",
        "Less sensitive to the number of clusters: Inertia naturally decreases as you increase the number of clusters (can favor too many clusters), while Silhouette Score balances compactness and separation to help identify a more meaningful number of clusters.\n",
        "\n",
        "In short:\n",
        "Silhouette Score provides a more holistic and interpretable measure of clustering quality by combining cluster cohesion and separation, making it more reliable than inertia alone.\n",
        "\n",
        "PRATICAL QUESTION\n",
        "Q21)Generate synthetic data with 4 centers using make_blobs and apply K-Means clustering. Visualize using a scatter plot\n",
        "Ans:\n",
        "\n",
        "\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate synthetic data with 4 centers\n",
        "X, y_true = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=42)\n",
        "\n",
        "# Apply K-Means clustering\n",
        "kmeans = KMeans(n_clusters=4, random_state=42)\n",
        "kmeans.fit(X)\n",
        "y_kmeans = kmeans.predict(X)\n",
        "\n",
        "# Visualize the clusters\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')\n",
        "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n",
        "            s=200, c='red', marker='X', label='Centroids')\n",
        "plt.title(\"K-Means Clustering on Synthetic Data\")\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "     \n",
        "\n",
        "Q22) Load the Iris dataset and use Agglomerative Clustering to group the data into 3 clusters. Display the first 10 predicted labels\n",
        "Ans:\n",
        "\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "\n",
        "# Apply Agglomerative Clustering\n",
        "agg_clustering = AgglomerativeClustering(n_clusters=3)\n",
        "labels = agg_clustering.fit_predict(X)\n",
        "\n",
        "# Display the first 10 predicted labels\n",
        "print(\"First 10 predicted cluster labels:\")\n",
        "print(labels[:10])\n",
        "\n",
        "     \n",
        "First 10 predicted cluster labels:\n",
        "[1 1 1 1 1 1 1 1 1 1]\n",
        "Q23)Generate synthetic data using make_moons and apply DBSCAN. Highlight outliers in the plot ?\n",
        "Ans:\n",
        "\n",
        "\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.cluster import DBSCAN\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate synthetic moon-shaped data\n",
        "X, _ = make_moons(n_samples=300, noise=0.1, random_state=42)\n",
        "\n",
        "# Apply DBSCAN\n",
        "dbscan = DBSCAN(eps=0.3, min_samples=5)\n",
        "labels = dbscan.fit_predict(X)\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(8, 6))\n",
        "# Core and border points\n",
        "plt.scatter(X[labels != -1, 0], X[labels != -1, 1],\n",
        "            c=labels[labels != -1], cmap='viridis', s=50, label='Clustered Points')\n",
        "\n",
        "# Outliers\n",
        "plt.scatter(X[labels == -1, 0], X[labels == -1, 1],\n",
        "            c='red', s=50, label='Outliers', marker='x')\n",
        "\n",
        "plt.title(\"DBSCAN on make_moons Data with Outliers Highlighted\")\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "     \n",
        "\n",
        "Q24)Load the Wine dataset and apply K-Means clustering after standardizing the features. Print the size of each cluster\n",
        "Ans:\n",
        "\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "\n",
        "# Load Wine dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply K-Means clustering\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "labels = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "# Print the size of each cluster\n",
        "unique, counts = np.unique(labels, return_counts=True)\n",
        "print(\"Cluster sizes:\")\n",
        "for cluster_id, size in zip(unique, counts):\n",
        "    print(f\"Cluster {cluster_id}: {size} samples\")\n",
        "\n",
        "     \n",
        "Cluster sizes:\n",
        "Cluster 0: 65 samples\n",
        "Cluster 1: 51 samples\n",
        "Cluster 2: 62 samples\n",
        "Q25)Use make_circles to generate synthetic data and cluster it using DBSCAN. Plot the result ?\n",
        "Ans:\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_circles\n",
        "from sklearn.cluster import DBSCAN\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Generate synthetic circular data\n",
        "X, y = make_circles(n_samples=500, factor=0.5, noise=0.05)\n",
        "\n",
        "# Step 2: Apply DBSCAN\n",
        "dbscan = DBSCAN(eps=0.2, min_samples=5)\n",
        "labels = dbscan.fit_predict(X)\n",
        "\n",
        "# Step 3: Plot the results\n",
        "plt.figure(figsize=(8, 6))\n",
        "unique_labels = set(labels)\n",
        "\n",
        "# Color map for clusters\n",
        "colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\n",
        "\n",
        "for label, col in zip(unique_labels, colors):\n",
        "    if label == -1:\n",
        "        # Black used for noise\n",
        "        col = [0, 0, 0, 1]\n",
        "    class_member_mask = (labels == label)\n",
        "    xy = X[class_member_mask]\n",
        "    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
        "             markeredgecolor='k', markersize=6)\n",
        "\n",
        "plt.title('DBSCAN Clustering on make_circles Data')\n",
        "plt.xlabel('X1')\n",
        "plt.ylabel('X2')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "     \n",
        "\n",
        "Q26) Load the Breast Cancer dataset, apply MinMaxScaler, and use K-Means with 2 clusters. Output the cluster centroids\n",
        "Ans:\n",
        "\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.cluster import KMeans\n",
        "import pandas as pd\n",
        "\n",
        "# Step 1: Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data  # Feature matrix\n",
        "\n",
        "# Step 2: Apply MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Step 3: Apply K-Means with 2 clusters\n",
        "kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
        "kmeans.fit(X_scaled)\n",
        "\n",
        "# Step 4: Output the cluster centroids\n",
        "centroids = kmeans.cluster_centers_\n",
        "\n",
        "# Optional: Create a DataFrame for easier reading\n",
        "centroid_df = pd.DataFrame(centroids, columns=data.feature_names)\n",
        "\n",
        "print(\"Cluster Centroids (scaled values):\")\n",
        "print(centroid_df)\n",
        "\n",
        "     \n",
        "Cluster Centroids (scaled values):\n",
        "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
        "0     0.504836      0.395603        0.505787   0.363766         0.469887   \n",
        "1     0.255354      0.288335        0.246964   0.143884         0.357431   \n",
        "\n",
        "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
        "0          0.422263        0.418387              0.46928       0.458997   \n",
        "1          0.180195        0.103448              0.13066       0.340118   \n",
        "\n",
        "   mean fractal dimension  ...  worst radius  worst texture  worst perimeter  \\\n",
        "0                0.299459  ...      0.480474       0.451074         0.465530   \n",
        "1                0.255916  ...      0.205241       0.320690         0.192421   \n",
        "\n",
        "   worst area  worst smoothness  worst compactness  worst concavity  \\\n",
        "0    0.314606          0.498688           0.363915         0.390273   \n",
        "1    0.099434          0.357112           0.148739         0.131423   \n",
        "\n",
        "   worst concave points  worst symmetry  worst fractal dimension  \n",
        "0              0.658272        0.337523                 0.260414  \n",
        "1              0.262314        0.226394                 0.154374  \n",
        "\n",
        "[2 rows x 30 columns]\n",
        "Q27)Generate synthetic data using make_blobs with varying cluster standard deviations and cluster with DBSCAN\n",
        "Ans:\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import DBSCAN\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Generate synthetic data with varying cluster standard deviations\n",
        "X, y = make_blobs(n_samples=500,\n",
        "                  centers=3,\n",
        "                  cluster_std=[0.5, 1.0, 2.5],  # Varying std deviation\n",
        "                  random_state=42)\n",
        "\n",
        "# Step 2: Apply DBSCAN\n",
        "dbscan = DBSCAN(eps=0.7, min_samples=5)  # Adjust `eps` based on cluster spread\n",
        "labels = dbscan.fit_predict(X)\n",
        "\n",
        "# Step 3: Plot the DBSCAN clustering results\n",
        "plt.figure(figsize=(8, 6))\n",
        "unique_labels = set(labels)\n",
        "\n",
        "# Assign colors to each cluster\n",
        "colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\n",
        "\n",
        "for label, col in zip(unique_labels, colors):\n",
        "    if label == -1:\n",
        "        # Black is used for noise.\n",
        "        col = [0, 0, 0, 1]\n",
        "    class_member_mask = (labels == label)\n",
        "    xy = X[class_member_mask]\n",
        "    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
        "             markeredgecolor='k', markersize=6)\n",
        "\n",
        "plt.title('DBSCAN Clustering on make_blobs Data (Varying std)')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "     \n",
        "\n",
        "Q28)Load the Digits dataset, reduce it to 2D using PCA, and visualize clusters from K-Means ?\n",
        "Ans:\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Step 1: Load the Digits dataset\n",
        "digits = load_digits()\n",
        "X = digits.data\n",
        "\n",
        "# Step 2: Reduce to 2D using PCA\n",
        "pca = PCA(n_components=2)\n",
        "X_reduced = pca.fit_transform(X)\n",
        "\n",
        "# Step 3: Apply K-Means clustering (10 clusters for digits 0‚Äì9)\n",
        "kmeans = KMeans(n_clusters=10, random_state=42, n_init=10)\n",
        "cluster_labels = kmeans.fit_predict(X_reduced)\n",
        "\n",
        "# Step 4: Visualize the clusters\n",
        "plt.figure(figsize=(10, 8))\n",
        "scatter = plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=cluster_labels, cmap='tab10', s=30, edgecolor='k')\n",
        "plt.title('K-Means Clustering on Digits Dataset (2D PCA)')\n",
        "plt.xlabel('PCA Component 1')\n",
        "plt.ylabel('PCA Component 2')\n",
        "plt.colorbar(scatter, label='Cluster ID')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "     \n",
        "\n",
        "Q29)Create synthetic data using make_blobs and evaluate silhouette scores for k = 2 to 5. Display as a bar chart ?\n",
        "Ans:\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Step 1: Generate synthetic data\n",
        "X, _ = make_blobs(n_samples=500, centers=4, cluster_std=1.0, random_state=42)\n",
        "\n",
        "# Step 2: Evaluate silhouette scores for k = 2 to 5\n",
        "k_values = range(2, 6)\n",
        "silhouette_scores = []\n",
        "\n",
        "for k in k_values:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    labels = kmeans.fit_predict(X)\n",
        "    score = silhouette_score(X, labels)\n",
        "    silhouette_scores.append(score)\n",
        "\n",
        "# Step 3: Plot the silhouette scores as a bar chart\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.bar(k_values, silhouette_scores, color='skyblue', edgecolor='black')\n",
        "plt.xlabel('Number of clusters (k)')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.title('Silhouette Scores for k = 2 to 5')\n",
        "plt.xticks(k_values)\n",
        "plt.grid(axis='y')\n",
        "plt.show()\n",
        "\n",
        "     \n",
        "\n",
        "Q30)Load the Iris dataset and use hierarchical clustering to group data. Plot a dendrogram with average linkage ?\n",
        "Ans:\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from scipy.cluster.hierarchy import linkage, dendrogram\n",
        "\n",
        "# Step 1: Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "feature_names = iris.feature_names\n",
        "\n",
        "# Step 2: Compute linkage matrix using average linkage\n",
        "linkage_matrix = linkage(X, method='average')\n",
        "\n",
        "# Step 3: Plot the dendrogram\n",
        "plt.figure(figsize=(12, 6))\n",
        "dendrogram(linkage_matrix,\n",
        "           labels=iris.target,\n",
        "           leaf_rotation=90,\n",
        "           leaf_font_size=10,\n",
        "           color_threshold=1.5)\n",
        "plt.title('Hierarchical Clustering Dendrogram (Average Linkage)')\n",
        "plt.xlabel('Sample Index / True Label')\n",
        "plt.ylabel('Distance')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "     \n",
        "\n",
        "Q31)Generate synthetic data with overlapping clusters using make_blobs, then apply K-Means and visualize with decision boundaries\n",
        "Ans:\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Generate synthetic data with overlapping clusters\n",
        "X, y_true = make_blobs(n_samples=500, centers=3, cluster_std=2.0, random_state=42)\n",
        "\n",
        "# Step 2: Apply K-Means clustering\n",
        "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
        "kmeans.fit(X)\n",
        "y_kmeans = kmeans.predict(X)\n",
        "\n",
        "# Step 3: Create a meshgrid for decision boundary visualization\n",
        "h = 0.05  # Step size of the mesh\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                     np.arange(y_min, y_max, h))\n",
        "\n",
        "# Predict cluster labels for each point in the mesh\n",
        "Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "# Step 4: Plot decision boundaries and clusters\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.contourf(xx, yy, Z, cmap='Pastel2', alpha=0.5)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, cmap='Set1', edgecolor='k', s=50)\n",
        "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n",
        "            s=200, c='black', marker='X', label='Centroids')\n",
        "plt.title('K-Means Clustering with Decision Boundaries')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "     \n",
        "\n",
        "Q32) Load the Digits dataset and apply DBSCAN after reducing dimensions with t-SNE. Visualize the results.\n",
        "Ans:\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "# Step 1: Load the Digits dataset\n",
        "digits = load_digits()\n",
        "X = digits.data\n",
        "\n",
        "# Step 2: Reduce to 2D using t-SNE\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
        "X_tsne = tsne.fit_transform(X)\n",
        "\n",
        "# Step 3: Apply DBSCAN\n",
        "dbscan = DBSCAN(eps=5, min_samples=5)  # You might need to tune eps\n",
        "labels = dbscan.fit_predict(X_tsne)\n",
        "\n",
        "# Step 4: Visualize the clustering result\n",
        "plt.figure(figsize=(10, 8))\n",
        "scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=labels, cmap='tab10', s=30, edgecolor='k')\n",
        "plt.title('DBSCAN Clustering on Digits Dataset (t-SNE reduced)')\n",
        "plt.xlabel('t-SNE Component 1')\n",
        "plt.ylabel('t-SNE Component 2')\n",
        "plt.colorbar(scatter, label='Cluster Label')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "     \n",
        "\n",
        "Q33)Generate synthetic data using make_blobs and apply Agglomerative Clustering with complete linkage. Plot the result\n",
        "Ans:\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "# Step 1: Generate synthetic data\n",
        "X, y_true = make_blobs(n_samples=500, centers=4, cluster_std=1.0, random_state=42)\n",
        "\n",
        "# Step 2: Apply Agglomerative Clustering with complete linkage\n",
        "agg_clustering = AgglomerativeClustering(n_clusters=4, linkage='complete')\n",
        "labels = agg_clustering.fit_predict(X)\n",
        "\n",
        "# Step 3: Plot the result\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='tab10', edgecolor='k', s=40)\n",
        "plt.title('Agglomerative Clustering (Complete Linkage)')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "     \n",
        "\n",
        "Q34)Load the Breast Cancer dataset and compare inertia values for K = 2 to 6 using K-Means. Show results in a line plot.\n",
        "Ans:\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Step 1: Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "\n",
        "# Optional: Scale features for better clustering performance\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Step 2: Compute inertia for K = 2 to 6\n",
        "k_values = range(2, 7)\n",
        "inertias = []\n",
        "\n",
        "for k in k_values:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    kmeans.fit(X_scaled)\n",
        "    inertias.append(kmeans.inertia_)\n",
        "\n",
        "# Step 3: Plot inertia vs K\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(k_values, inertias, marker='o', linestyle='-', color='blue')\n",
        "plt.title('K-Means Inertia for Breast Cancer Dataset')\n",
        "plt.xlabel('Number of clusters (K)')\n",
        "plt.ylabel('Inertia (Sum of squared distances)')\n",
        "plt.xticks(k_values)\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "     \n",
        "\n",
        "Q35) Generate synthetic concentric circles using make_circles and cluster using Agglomerative Clustering with single linkage\n",
        "Ans:\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_circles\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "# Step 1: Generate concentric circles data\n",
        "X, y_true = make_circles(n_samples=500, factor=0.5, noise=0.05, random_state=42)\n",
        "\n",
        "# Step 2: Apply Agglomerative Clustering with single linkage\n",
        "agg_clustering = AgglomerativeClustering(n_clusters=2, linkage='single')\n",
        "labels = agg_clustering.fit_predict(X)\n",
        "\n",
        "# Step 3: Plot the clusters\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='Set1', edgecolor='k', s=50)\n",
        "plt.title('Agglomerative Clustering with Single Linkage on Concentric Circles')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "     \n",
        "\n",
        "Q36)Use the Wine dataset, apply DBSCAN after scaling the data, and count the number of clusters (excluding noise)\n",
        "Ans:\n",
        "\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import DBSCAN\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "\n",
        "# Step 2: Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Step 3: Apply DBSCAN\n",
        "dbscan = DBSCAN(eps=1.5, min_samples=5)  # eps may need tuning\n",
        "labels = dbscan.fit_predict(X_scaled)\n",
        "\n",
        "# Step 4: Count clusters excluding noise (-1)\n",
        "n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "\n",
        "print(f\"Number of clusters found (excluding noise): {n_clusters}\")\n",
        "\n",
        "     \n",
        "Number of clusters found (excluding noise): 0\n",
        "Q37)Generate synthetic data with make_blobs and apply KMeans. Then plot the cluster centers on top of the data points.\n",
        "Ans:\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Step 1: Generate synthetic data\n",
        "X, y_true = make_blobs(n_samples=500, centers=4, cluster_std=1.0, random_state=42)\n",
        "\n",
        "# Step 2: Apply KMeans clustering\n",
        "kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)\n",
        "kmeans.fit(X)\n",
        "centers = kmeans.cluster_centers_\n",
        "labels = kmeans.labels_\n",
        "\n",
        "# Step 3: Plot data points and cluster centers\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=30, edgecolor='k', alpha=0.6)\n",
        "plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, marker='X', label='Cluster Centers')\n",
        "plt.title('K-Means Clustering with Cluster Centers')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "     \n",
        "\n",
        "Q38)Load the Iris dataset, cluster with DBSCAN, and print how many samples were identified as noise.\n",
        "Ans:\n",
        "\n",
        "\n",
        "\n",
        "     \n",
        "Number of samples identified as noise: 34\n",
        "Q39)Generate synthetic non-linearly separable data using make_moons, apply K-Means, and visualize the clustering result.\n",
        "Ans:\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Step 1: Generate make_moons data\n",
        "X, y_true = make_moons(n_samples=500, noise=0.1, random_state=42)\n",
        "\n",
        "# Step 2: Apply K-Means clustering\n",
        "kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
        "labels = kmeans.fit_predict(X)\n",
        "\n",
        "# Step 3: Visualize clusters\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=40, edgecolor='k')\n",
        "plt.title('K-Means Clustering on Non-linearly Separable make_moons Data')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "     \n",
        "\n",
        "Q40)Load the Digits dataset, apply PCA to reduce to 3 components, then use KMeans and visualize with a 3D scatter plot.\n",
        "Ans:\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "from mpl_toolkits.mplot3d import Axes3D  # Needed for 3D plotting\n",
        "\n",
        "# Step 1: Load Digits dataset\n",
        "digits = load_digits()\n",
        "X = digits.data\n",
        "\n",
        "# Step 2: Reduce to 3D using PCA\n",
        "pca = PCA(n_components=3)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "# Step 3: Apply KMeans clustering (10 clusters for digits 0-9)\n",
        "kmeans = KMeans(n_clusters=10, random_state=42, n_init=10)\n",
        "labels = kmeans.fit_predict(X_pca)\n",
        "\n",
        "# Step 4: Visualize with 3D scatter plot\n",
        "fig = plt.figure(figsize=(10, 8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2],\n",
        "                     c=labels, cmap='tab10', s=50, edgecolor='k')\n",
        "\n",
        "ax.set_title('K-Means Clustering on Digits Dataset (3D PCA)')\n",
        "ax.set_xlabel('PCA Component 1')\n",
        "ax.set_ylabel('PCA Component 2')\n",
        "ax.set_zlabel('PCA Component 3')\n",
        "\n",
        "plt.legend(*scatter.legend_elements(), title=\"Clusters\", loc='best')\n",
        "plt.show()\n",
        "\n",
        "     \n"
      ],
      "metadata": {
        "id": "yHej6Nx8en6q"
      }
    }
  ]
}