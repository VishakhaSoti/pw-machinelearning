{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1 What is a Support Vector Machine (SVM)?\n",
        "\n",
        "SVM is a supervised learning algorithm used for classification and regression tasks. It aims to find the hyperplane (decision boundary) that best separates data points of different classes by maximizing the margin between them.\n",
        "\n",
        "2 What is the difference between Hard Margin and Soft Margin SVM?\n",
        "\n",
        "Hard Margin SVM: Used when the data is linearly separable. It creates a decision boundary where no data points are allowed on the wrong side of the hyperplane. Soft Margin SVM: Used when the data is not perfectly separable. It allows some misclassification of data points but tries to balance maximizing the margin and minimizing the classification error, controlled by a parameter\n",
        "\n",
        "**3 What is the mathematical intuition behind SVM? **\n",
        "\n",
        "SVM works by transforming the data into a higher-dimensional space where a hyperplane can be used to separate the classes. The goal is to find the hyperplane that maximizes the margin, defined as the distance between the hyperplane and the closest points from each class (the support vectors).\n",
        "\n",
        "4 What is the role of Lagrange Multipliers in SVM?\n",
        "\n",
        "Lagrange multipliers are used to convert the constrained optimization problem in SVM (maximizing the margin subject to constraints that points must be classified correctly) into an unconstrained one. This allows for the use of optimization techniques to find the optimal hyperplane.\n",
        "\n",
        "5 What are Support Vectors in SVM?\n",
        "\n",
        "Support vectors are the data points that are closest to the decision boundary (hyperplane). These points are critical as they define the position and orientation of the hyperplane. They are the ones that determine the margin of the SVM model.\n",
        "\n",
        "6 What is a Support Vector Classifier (SVC)?\n",
        "\n",
        "SVC is a specific implementation of SVM for classification tasks. It learns the optimal hyperplane that maximizes the margin between classes, making it suitable for binary and multiclass classification.\n",
        "\n",
        "7 What is a Support Vector Regressor (SVR)?\n",
        "\n",
        "SVR is a variant of SVM used for regression tasks. Instead of separating data into classes, it tries to fit a function that has at most a certain margin of tolerance (controlled by a parameter ùúñ œµ) between the actual data points and the predicted values.\n",
        "\n",
        "8 What is the Kernel Trick in SVM?\n",
        "\n",
        "The kernel trick allows SVM to operate in higher-dimensional feature spaces without explicitly computing the coordinates of the data in that space. This is done by using a kernel function (such as the polynomial or radial basis function (RBF) kernel) to calculate the dot product in the higher-dimensional space.\n",
        "\n",
        "9 Compare Linear Kernel, Polynomial Kernel, and RBF Kernel:\n",
        "\n",
        "Linear Kernel: Suitable for linearly separable data. It computes the dot product of input vectors directly.\n",
        "\n",
        "Polynomial Kernel: Useful for non-linear data. It computes the dot product raised to a power d, allowing for curved decision boundaries.\n",
        "\n",
        "RBF Kernel: The most commonly used kernel in SVM. It projects data into an infinite-dimensional space, allowing complex decision boundaries. It computes the similarity between data points based on their Euclidean distance.\n",
        "\n",
        "10 What is the effect of the C parameter in SVM?\n",
        "\n",
        "The C parameter controls the trade-off between maximizing the margin and minimizing classification error. A high value of C tries to classify all training points correctly, possibly leading to overfitting. A lower C allows more misclassification but can improve generalization.\n",
        "\n",
        "11 What is the role of the Gamma parameter in RBF Kernel SVM?\n",
        "\n",
        "The Gamma parameter in RBF kernel determines the influence of a single training example. A small value of gamma means a far-reaching influence, while a large gamma means that each training point has a small influence, leading to a more complex decision boundary.\n",
        "\n",
        "12 What is the Na√Øve Bayes classifier, and why is it called \"Na√Øve\"\n",
        "\n",
        "Na√Øve Bayes is a classification algorithm based on Bayes' Theorem that assumes that features are conditionally independent given the class. The term \"na√Øve\" comes from this assumption, which is often unrealistic but still works well in many cases, especially for text classification.\n",
        "\n",
        "13 What is Bayes‚Äô Theorem?\n",
        "\n",
        "Bayes' Theorem describes the probability of a class ùê∂ C given observed features ùëã X using the formula: ùëÉ ( ùê∂ ‚à£ ùëã )\n",
        "ùëÉ ( ùëã ‚à£ ùê∂ ) ùëÉ ( ùê∂ ) ùëÉ ( ùëã ) P(C‚à£X)= P(X) P(X‚à£C)P(C)‚Äã\n",
        "\n",
        "where ùëÉ ( ùê∂ ‚à£ ùëã ) P(C‚à£X) is the posterior probability of the class given the features, ùëÉ ( ùëã ‚à£ ùê∂ ) P(X‚à£C) is the likelihood, ùëÉ ( ùê∂ ) P(C) is the prior probability of the class, and ùëÉ ( ùëã ) P(X) is the evidence.\n",
        "\n",
        "**14 Explain the differences between Gaussian Na√Øve BayesMultinomial Na√Øve Bayes, and Bernoulli Na√Øve Bayes:\n",
        "\n",
        "Gaussian Na√Øve Bayes: Assumes that the features follow a Gaussian (normal) distribution.\n",
        "\n",
        "Multinomial Na√Øve Bayes: Assumes that the features (especially used for text classification) are counts or frequencies of events (e.g., word counts in text).\n",
        "\n",
        "Bernoulli Na√Øve Bayes: Assumes that the features are binary (e.g., the presence or absence of a word in text classification).\n",
        "\n",
        "15 When should you use Gaussian Na√Øve Bayes over other variants?\n",
        "\n",
        "Gaussian Na√Øve Bayes is ideal when the features are continuous and can be modeled using a Gaussian distribution. It is suitable for cases where the feature values are real numbers, such as in medical data with continuous attributes.\n",
        "\n",
        "16 What are the key assumptions made by Na√Øve Bayes?\n",
        "\n",
        "The key assumption in Na√Øve Bayes is the conditional independence of features given the class. This assumption simplifies the computation of the likelihood term ùëÉ ( ùëã ‚à£ ùê∂ ) P(X‚à£C) as the product of individual feature probabilities.\n",
        "\n",
        "17 What are the advantages and disadvantages of Na√Øve Bayes?\n",
        "\n",
        "Advantages:\n",
        "Simple and easy to implement. Efficient with large datasets. Works well for text classification (e.g., spam detection).\n",
        "\n",
        "Disadvantages:\n",
        "The conditional independence assumption is often unrealistic. It doesn't work well when features are highly correlated.\n",
        "\n",
        "18 Why is Na√Øve Bayes a good choice for text classification?\n",
        "\n",
        "Na√Øve Bayes works well for text classification because it models the conditional independence between words in the documents, which makes it computationally efficient and effective for tasks like spam detection or sentiment analysis, even with large vocabularies.\n",
        "\n",
        "19 Compare SVM and Na√Øve Bayes for classification tasks:\n",
        "\n",
        "SVM is more powerful when the decision boundary is complex and non-linear. It works well for small to medium-sized datasets with many features, especially when there is a clear margin of separation.\n",
        "\n",
        "Na√Øve Bayes, on the other hand, is more efficient and works well when the features are independent, and the data is relatively simple. It is often used in text classification due to its simplicity and speed.\n",
        "\n",
        "20 How does Laplace Smoothing help in Na√Øve Bayes?\n",
        "\n",
        "Laplace Smoothing (also called additive smoothing) is used in Na√Øve Bayes to handle the problem of zero probabilities when a feature value does not appear in the training set for a particular class. It adds a small constant (usually 1) to the count of each feature to ensure that all features have a non-zero probability.\n",
        "\n",
        "\n",
        "# 21  Write a Python program to train an SVM Classifier on the Iris dataset and evaluate accuracy:\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train an SVM Classifier with a Linear Kernel\n",
        "svm_clf = SVC(kernel='linear')\n",
        "svm_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = svm_clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"SVM Classifier Accuracy with Linear Kernel: {accuracy:.4f}\")\n",
        "\n",
        "\n",
        "     \n",
        "SVM Classifier Accuracy with Linear Kernel: 1.0000\n",
        "\n",
        "# 22 Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then\n",
        "# compare their accuracies:\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Wine dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train an SVM Classifier with Linear Kernel\n",
        "svm_linear = SVC(kernel='linear')\n",
        "svm_linear.fit(X_train, y_train)\n",
        "\n",
        "# Train an SVM Classifier with RBF Kernel\n",
        "svm_rbf = SVC(kernel='rbf')\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and evaluate accuracy for Linear Kernel\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
        "\n",
        "# Make predictions and evaluate accuracy for RBF Kernel\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "accuracy_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "# Print the accuracies\n",
        "print(f\"SVM Accuracy with Linear Kernel: {accuracy_linear:.4f}\")\n",
        "print(f\"SVM Accuracy with RBF Kernel: {accuracy_rbf:.4f}\")\n",
        "\n",
        "     \n",
        "SVM Accuracy with Linear Kernel: 0.9815\n",
        "SVM Accuracy with RBF Kernel: 0.7593\n",
        "\n",
        "# 24 Write a Python program to train an SVM Classifier with a Polynomial Kernel and visualize the decision\n",
        "# boundary:\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Generate a synthetic 2D dataset\n",
        "# Set n_informative, n_redundant, and n_repeated to values that sum to less than n_features\n",
        "X, y = make_classification(n_samples=100, n_features=2, n_classes=2, n_informative=2, n_redundant=0, n_repeated=0, random_state=42)\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Standardize the features (important for SVMs)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train an SVM Classifier with Polynomial Kernel\n",
        "svm_poly = SVC(kernel='poly', degree=3)\n",
        "svm_poly.fit(X_train, y_train)\n",
        "\n",
        "# Visualize the decision boundary\n",
        "xx, yy = np.meshgrid(np.linspace(X_train[:, 0].min(), X_train[:, 0].max(), 100),\n",
        "                     np.linspace(X_train[:, 1].min(), X_train[:, 1].max(), 100))\n",
        "Z = svm_poly.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "plt.contourf(xx, yy, Z, alpha=0.75, cmap='coolwarm')\n",
        "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, edgecolors='k', marker='o', cmap='coolwarm')\n",
        "plt.title('SVM with Polynomial Kernel - Decision Boundary')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.show()\n",
        "     \n",
        "\n",
        "\n",
        "# 25 Write a Python program to train a Gaussian Na√Øve Bayes classifier on the Breast Cancer dataset and\n",
        "# evaluate accuracy:\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a Gaussian Na√Øve Bayes classifier\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = gnb.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Gaussian Na√Øve Bayes Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "     \n",
        "Gaussian Na√Øve Bayes Accuracy: 0.9415\n",
        "\n",
        "# 26 Write a Python program to train a Multinomial Na√Øve Bayes classifier for text classification using the 20\n",
        "# Newsgroups dataset\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load 20 Newsgroups dataset\n",
        "newsgroups = fetch_20newsgroups(subset='all')\n",
        "\n",
        "# Convert the text data into TF-IDF features\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X = vectorizer.fit_transform(newsgroups.data)\n",
        "y = newsgroups.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a Multinomial Na√Øve Bayes classifier\n",
        "mnb = MultinomialNB()\n",
        "mnb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = mnb.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Multinomial Na√Øve Bayes Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "     \n",
        "Multinomial Na√Øve Bayes Accuracy: 0.8721\n",
        "\n",
        "# 28 Write a Python program to train a Bernoulli Na√Øve Bayes classifier for binary classification on a dataset with\n",
        "# binary features\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate synthetic binary classification dataset with binary features\n",
        "X, y = make_classification(n_samples=100, n_features=10, n_informative=5, n_classes=2, random_state=42,\n",
        "                            n_clusters_per_class=1, flip_y=0, class_sep=2)\n",
        "\n",
        "# Convert the dataset to binary features\n",
        "X = (X > 0).astype(int)  # Convert to binary features (0 or 1)\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a Bernoulli Na√Øve Bayes classifier\n",
        "bnb = BernoulliNB()\n",
        "bnb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = bnb.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Bernoulli Na√Øve Bayes Classifier Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "     \n",
        "Bernoulli Na√Øve Bayes Classifier Accuracy: 1.0000\n",
        "\n",
        "# 29 Write a Python program to apply feature scaling before training an SVM model and compare results with\n",
        "# unscaled data\n",
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train an SVM Classifier without scaling the data\n",
        "svm_clf_unscaled = SVC(kernel='linear')\n",
        "svm_clf_unscaled.fit(X_train, y_train)\n",
        "y_pred_unscaled = svm_clf_unscaled.predict(X_test)\n",
        "accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
        "\n",
        "# Apply feature scaling (Standardization)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train an SVM Classifier with scaled data\n",
        "svm_clf_scaled = SVC(kernel='linear')\n",
        "svm_clf_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = svm_clf_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# Print the accuracies\n",
        "print(f\"Accuracy without scaling: {accuracy_unscaled:.4f}\")\n",
        "print(f\"Accuracy with scaling: {accuracy_scaled:.4f}\")\n",
        "\n",
        "     \n",
        "Accuracy without scaling: 1.0000\n",
        "Accuracy with scaling: 0.9778\n",
        "\n",
        "# 30 Write a Python program to train a Gaussian Na√Øve Bayes model and compare the predictions before and\n",
        "# after Laplace Smoothing\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Gaussian Na√Øve Bayes without Laplace smoothing (default)\n",
        "gnb_no_smoothing = GaussianNB(var_smoothing=1e-9)  # Small variance to avoid underflow (no smoothing)\n",
        "gnb_no_smoothing.fit(X_train, y_train)\n",
        "y_pred_no_smoothing = gnb_no_smoothing.predict(X_test)\n",
        "\n",
        "# Train Gaussian Na√Øve Bayes with Laplace smoothing (default var_smoothing=1.0)\n",
        "gnb_with_smoothing = GaussianNB(var_smoothing=1.0)  # Laplace smoothing\n",
        "gnb_with_smoothing.fit(X_train, y_train)\n",
        "y_pred_with_smoothing = gnb_with_smoothing.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy_no_smoothing = accuracy_score(y_test, y_pred_no_smoothing)\n",
        "accuracy_with_smoothing = accuracy_score(y_test, y_pred_with_smoothing)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Accuracy without Laplace smoothing: {accuracy_no_smoothing:.4f}\")\n",
        "print(f\"Accuracy with Laplace smoothing: {accuracy_with_smoothing:.4f}\")\n",
        "\n",
        "     \n",
        "Accuracy without Laplace smoothing: 0.9778\n",
        "Accuracy with Laplace smoothing: 0.9778\n",
        "\n",
        "# 31 Write a Python program to train an SVM Classifier and use GridSearchCV to tune the hyperparameters (C,\n",
        "# gamma, kernel)\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define the SVM model\n",
        "svm = SVC()\n",
        "\n",
        "# Define the parameter grid for C, gamma, and kernel\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'gamma': ['scale', 'auto', 0.1, 1],\n",
        "    'kernel': ['linear', 'rbf']\n",
        "}\n",
        "\n",
        "# Set up GridSearchCV with 5-fold cross-validation\n",
        "grid_search = GridSearchCV(svm, param_grid, cv=5, verbose=1)\n",
        "\n",
        "# Fit the grid search model\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters and best score\n",
        "print(\"Best parameters found: \", grid_search.best_params_)\n",
        "print(\"Best cross-validation score: \", grid_search.best_score_)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "y_pred = grid_search.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test accuracy: {accuracy:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "     \n",
        "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
        "Best parameters found:  {'C': 1, 'gamma': 'scale', 'kernel': 'linear'}\n",
        "Best cross-validation score:  0.9619047619047618\n",
        "Test accuracy: 1.0000\n",
        "\n",
        "# 32 Write a Python program to train an SVM Classifier on an imbalanced dataset and apply class weighting and\n",
        "# check it improve accuracy\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate an imbalanced dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=2, n_classes=2,\n",
        "                           weights=[0.9, 0.1], flip_y=0, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train an SVM classifier without class weighting\n",
        "svm_no_weight = SVC(kernel='linear')\n",
        "svm_no_weight.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_no_weight = svm_no_weight.predict(X_test)\n",
        "\n",
        "# Train an SVM classifier with class weighting\n",
        "svm_with_weight = SVC(kernel='linear', class_weight='balanced')\n",
        "svm_with_weight.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_with_weight = svm_with_weight.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy_no_weight = accuracy_score(y_test, y_pred_no_weight)\n",
        "accuracy_with_weight = accuracy_score(y_test, y_pred_with_weight)\n",
        "\n",
        "print(f\"Accuracy without class weighting: {accuracy_no_weight:.4f}\")\n",
        "print(f\"Accuracy with class weighting: {accuracy_with_weight:.4f}\")\n",
        "\n",
        "     \n",
        "Accuracy without class weighting: 0.9400\n",
        "Accuracy with class weighting: 0.8600\n",
        "\n",
        "# 33 Write a Python program to implement a Na√Øve Bayes classifier for spam detection using email data\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Sample email dataset (spam and ham)\n",
        "emails = [\n",
        "    \"Free money now!!!\", \"Hi, I wanted to check on the report\", \"Congratulations, you won a prize\",\n",
        "    \"Hello, how are you?\", \"Get a free iPhone by clicking here\", \"Important meeting tomorrow\"\n",
        "]\n",
        "labels = [1, 0, 1, 0, 1, 0]  # 1 for spam, 0 for ham (non-spam)\n",
        "\n",
        "# Convert emails to word counts (Bag of Words model)\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(emails)\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a Na√Øve Bayes classifier\n",
        "nb_clf = MultinomialNB()\n",
        "nb_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = nb_clf.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Spam detection accuracy: {accuracy:.4f}\")\n",
        "\n",
        "     \n",
        "Spam detection accuracy: 1.0000\n",
        "\n",
        "# 34 Write a Python program to train an SVM Classifier and a Na√Øve Bayes Classifier on the same dataset and\n",
        "# compare their accuracy\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train an SVM classifier\n",
        "svm_clf = SVC(kernel='linear')\n",
        "svm_clf.fit(X_train, y_train)\n",
        "y_pred_svm = svm_clf.predict(X_test)\n",
        "\n",
        "# Train a Na√Øve Bayes classifier\n",
        "nb_clf = GaussianNB()\n",
        "nb_clf.fit(X_train, y_train)\n",
        "y_pred_nb = nb_clf.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
        "accuracy_nb = accuracy_score(y_test, y_pred_nb)\n",
        "\n",
        "print(f\"SVM Classifier accuracy: {accuracy_svm:.4f}\")\n",
        "print(f\"Na√Øve Bayes Classifier accuracy: {accuracy_nb:.4f}\")\n",
        "\n",
        "     \n",
        "SVM Classifier accuracy: 1.0000\n",
        "Na√Øve Bayes Classifier accuracy: 0.9778\n",
        "\n",
        "# 35 Write a Python program to perform feature selection before training a Na√Øve Bayes classifier and compare\n",
        "# results\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Perform feature selection using SelectKBest (select top 2 features)\n",
        "selector = SelectKBest(f_classif, k=2)\n",
        "X_train_selected = selector.fit_transform(X_train, y_train)\n",
        "X_test_selected = selector.transform(X_test)\n",
        "\n",
        "# Train a Na√Øve Bayes classifier on selected features\n",
        "nb_clf = GaussianNB()\n",
        "nb_clf.fit(X_train_selected, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = nb_clf.predict(X_test_selected)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy_selected = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Train Na√Øve Bayes without feature selection\n",
        "nb_clf_no_selection = GaussianNB()\n",
        "nb_clf_no_selection.fit(X_train, y_train)\n",
        "y_pred_no_selection = nb_clf_no_selection.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy without feature selection\n",
        "accuracy_no_selection = accuracy_score(y_test, y_pred_no_selection)\n",
        "\n",
        "print(f\"Accuracy with feature selection: {accuracy_selected:.4f}\")\n",
        "print(f\"Accuracy without feature selection: {accuracy_no_selection:.4f}\")\n",
        "\n",
        "     \n",
        "Accuracy with feature selection: 1.0000\n",
        "Accuracy without feature selection: 0.9778\n",
        "\n",
        "# 36 Write a Python program to train an SVM Classifier using One-vs-Rest (OvR) and One-vs-One (OvO)\n",
        "# strategies on the Wine dataset and compare their accuracy\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
        "\n",
        "# Load the Wine dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# One-vs-Rest strategy\n",
        "ovr_clf = OneVsRestClassifier(SVC(kernel='linear'))\n",
        "ovr_clf.fit(X_train, y_train)\n",
        "y_pred_ovr = ovr_clf.predict(X_test)\n",
        "accuracy_ovr = accuracy_score(y_test, y_pred_ovr)\n",
        "\n",
        "# One-vs-One strategy\n",
        "ovo_clf = OneVsOneClassifier(SVC(kernel='linear'))\n",
        "ovo_clf.fit(X_train, y_train)\n",
        "y_pred_ovo = ovo_clf.predict(X_test)\n",
        "accuracy_ovo = accuracy_score(y_test, y_pred_ovo)\n",
        "\n",
        "# Print accuracies\n",
        "print(f\"Accuracy with One-vs-Rest: {accuracy_ovr:.4f}\")\n",
        "print(f\"Accuracy with One-vs-One: {accuracy_ovo:.4f}\")\n",
        "\n",
        "\n",
        "     \n",
        "Accuracy with One-vs-Rest: 0.9815\n",
        "Accuracy with One-vs-One: 0.9815\n",
        "\n",
        "# 37 Write a Python program to train an SVM Classifier using Linear, Polynomial, and RBF kernels on the Breast\n",
        "# Cancer dataset and compare their accuracy\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Linear Kernel SVM\n",
        "svm_linear = SVC(kernel='linear')\n",
        "svm_linear.fit(X_train, y_train)\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
        "\n",
        "# Polynomial Kernel SVM\n",
        "svm_poly = SVC(kernel='poly', degree=3)\n",
        "svm_poly.fit(X_train, y_train)\n",
        "y_pred_poly = svm_poly.predict(X_test)\n",
        "accuracy_poly = accuracy_score(y_test, y_pred_poly)\n",
        "\n",
        "# RBF Kernel SVM\n",
        "svm_rbf = SVC(kernel='rbf')\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "accuracy_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "# Print accuracies\n",
        "print(f\"Accuracy with Linear Kernel: {accuracy_linear:.4f}\")\n",
        "print(f\"Accuracy with Polynomial Kernel: {accuracy_poly:.4f}\")\n",
        "print(f\"Accuracy with RBF Kernel: {accuracy_rbf:.4f}\")\n",
        "\n",
        "     \n",
        "Accuracy with Linear Kernel: 0.9649\n",
        "Accuracy with Polynomial Kernel: 0.9415\n",
        "Accuracy with RBF Kernel: 0.9357\n",
        "\n",
        "# 38 Write a Python program to train an SVM Classifier using Stratified K-Fold Cross-Validation and compute the\n",
        "# average accuracy\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Initialize the SVM classifier\n",
        "svm = SVC(kernel='linear')\n",
        "\n",
        "# Use Stratified K-Fold Cross-Validation\n",
        "skf = StratifiedKFold(n_splits=5)\n",
        "cross_val_scores = cross_val_score(svm, X, y, cv=skf)\n",
        "\n",
        "# Print the average accuracy\n",
        "print(f\"Average accuracy with Stratified K-Fold Cross-Validation: {cross_val_scores.mean():.4f}\")\n",
        "\n",
        "     \n",
        "Average accuracy with Stratified K-Fold Cross-Validation: 0.9455\n",
        "\n",
        "# 39 Write a Python program to train a Na√Øve Bayes classifier using different prior probabilities and compare\n",
        "# performance\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Na√Øve Bayes classifier with different priors\n",
        "nb_clf_default = GaussianNB()\n",
        "nb_clf_default.fit(X_train, y_train)\n",
        "y_pred_default = nb_clf_default.predict(X_test)\n",
        "accuracy_default = accuracy_score(y_test, y_pred_default)\n",
        "\n",
        "# Train Na√Øve Bayes classifier with custom priors\n",
        "nb_clf_custom = GaussianNB(priors=[0.4, 0.3, 0.3])\n",
        "nb_clf_custom.fit(X_train, y_train)\n",
        "y_pred_custom = nb_clf_custom.predict(X_test)\n",
        "accuracy_custom = accuracy_score(y_test, y_pred_custom)\n",
        "\n",
        "# Print accuracies\n",
        "print(f\"Accuracy with default priors: {accuracy_default:.4f}\")\n",
        "print(f\"Accuracy with custom priors: {accuracy_custom:.4f}\")\n",
        "\n",
        "     \n",
        "Accuracy with default priors: 0.9778\n",
        "Accuracy with custom priors: 0.9778"
      ],
      "metadata": {
        "id": "yHej6Nx8en6q"
      }
    }
  ]
}