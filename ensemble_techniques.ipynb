{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. Can we use Bagging for regression problems?\n",
        "\n",
        "Yes, Bagging (Bootstrap Aggregating) can be used for regression problems. In Bagging, multiple models are trained on different subsets of the data (using bootstrapping), and their predictions are averaged (for regression) or voted on (for classification) to make the final prediction.\n",
        "\n",
        "2. What is the difference between multiple model training and single model training?\n",
        "\n",
        "Single Model Training involves training one model on the entire dataset. The model is then evaluated based on its performance. Multiple Model Training involves training multiple models, often with different subsets of the data (like in Bagging) or using different algorithms (like in ensemble methods). This helps improve the model’s generalization and performance by combining the outputs of multiple models.\n",
        "\n",
        "3. Explain the concept of feature randomness in Random Forest\n",
        "\n",
        "In Random Forest, feature randomness refers to the process where, at each split in a tree, a random subset of features is considered rather than all features. This randomness helps in decorrelating the trees, which results in a more robust and diverse set of decision trees that can reduce overfitting.\n",
        "\n",
        "**4. What is OOB (Out-of-Bag) Score? **\n",
        "\n",
        "The Out-of-Bag (OOB) Score is an internal cross-validation method used in ensemble techniques like Bagging and Random Forest. For each data point, it is not included in the bootstrap sample used to train a particular tree, and these \"out-of-bag\" data points are used to estimate the performance of the model. The OOB score provides an unbiased estimate of the model's accuracy without needing a separate validation set.\n",
        "\n",
        "5. How can you measure the importance of features in a Random Forest model?\n",
        "\n",
        "The importance of features in a Random Forest model can be measured by looking at the decrease in impurity (like Gini impurity or entropy) when a feature is used to split the data. The feature importance can also be assessed by the mean decrease in accuracy when a feature is permuted or removed from the model.\n",
        "\n",
        "**6. Explain the working principle of a Bagging Classifier A Bagging Classifier works by **\n",
        "\n",
        "Generating multiple bootstrap samples (random subsets with replacement) from the training data. Training a separate classifier (e.g., Decision Trees) on each of these subsets.\n",
        "\n",
        "Aggregating the predictions of all classifiers, typically by voting for classification tasks (majority vote) to determine the final prediction.\n",
        "\n",
        "7. How do you evaluate a Bagging Classifier’s performance?\n",
        "\n",
        "You can evaluate a Bagging Classifier's performance using standard metrics such as accuracy, precision, recall, F1-score, and ROC-AUC for classification tasks. For regression tasks, metrics like mean squared error (MSE) or mean absolute error (MAE) are used.\n",
        "\n",
        "8. How does a Bagging Regressor work?\n",
        "\n",
        "A Bagging Regressor works similarly to a Bagging Classifier:\n",
        "\n",
        "Multiple bootstrap samples of the training data are created. A regressor (e.g., decision trees) is trained on each bootstrap sample. The final prediction is the average of the predictions from all the individual regressors.\n",
        "\n",
        "9. What is the main advantage of ensemble techniques?\n",
        "\n",
        "The main advantage of ensemble techniques is that they combine the predictions of multiple models, which generally leads to better performance and more robust predictions than using a single model. They help reduce overfitting and increase generalization.\n",
        "\n",
        "10. What is the main challenge of ensemble methods?\n",
        "\n",
        "The main challenge of ensemble methods is that they can be computationally expensive due to the need to train multiple models, especially for large datasets. Also, ensemble models are typically harder to interpret compared to individual models.\n",
        "\n",
        "11. Explain the key idea behind ensemble techniques\n",
        "\n",
        "The key idea behind ensemble techniques is to combine the outputs of multiple models to improve the overall performance. The individual models (often weak learners) are combined in such a way that their collective prediction is more accurate and robust than any single model.\n",
        "\n",
        "12. What is a Random Forest Classifier?\n",
        "\n",
        "A Random Forest Classifier is an ensemble learning algorithm that creates multiple decision trees using bootstrapped samples and random feature selection at each split. The final classification decision is made by taking a majority vote from all the decision trees in the forest.\n",
        "\n",
        "13. What are the main types of ensemble techniques?\n",
        "\n",
        "The main types of ensemble techniques are:\n",
        "\n",
        "Bagging (Bootstrap Aggregating) — like Random Forest. Boosting — like AdaBoost, Gradient Boosting, and XGBoost. Stacking — combining different models and using another model to make predictions from the output of the base models.\n",
        "\n",
        "14. What is ensemble learning in machine learning?\n",
        "\n",
        "Ensemble learning is a technique in machine learning where multiple models are trained and combined to produce better predictions. The idea is to improve the overall performance and reduce errors by leveraging the strengths of different models.\n",
        "\n",
        "15. When should we avoid using ensemble methods?\n",
        "\n",
        "Ensemble methods should be avoided when:\n",
        "\n",
        "Interpretability is crucial, as ensemble models (like Random Forest or Boosting) are complex and harder to interpret. There is limited computational power, as training multiple models can be resource-intensive. The dataset is too small, and overfitting is more likely to occur.\n",
        "\n",
        "16. How does Bagging help in reducing overfitting?\n",
        "\n",
        "Bagging helps in reducing overfitting by averaging the predictions of multiple models, which reduces the variance and makes the model more stable. Since each model is trained on a different subset of the data, the overall model is less likely to overfit to specific noise or patterns in the training set.\n",
        "\n",
        "17. Why is Random Forest better than a single Decision Tree?\n",
        "\n",
        "A Random Forest is better than a single Decision Tree because:\n",
        "\n",
        "It reduces overfitting by combining multiple decision trees, each trained on different bootstrapped data. It decorrelates the trees by using random feature selection at each split, improving the generalization power. A Random Forest is generally more robust and performs better in terms of accuracy.\n",
        "\n",
        "18. What is the role of bootstrap sampling in Bagging?\n",
        "\n",
        "In Bagging, bootstrap sampling involves creating multiple subsets of the training data by sampling with replacement. Each subset is used to train a different model. This allows the algorithm to reduce variance and improve generalization by averaging out predictions from different models.\n",
        "\n",
        "19. What are some real-world applications of ensemble techniques?\n",
        "\n",
        "Some real-world applications of ensemble techniques include:\n",
        "\n",
        "Spam detection (e.g., Random Forests for classifying emails). Credit scoring (using Gradient Boosting or Random Forest for predicting creditworthiness). Medical diagnostics (e.g., using ensemble models for predicting diseases like cancer). Recommendation systems (combining various models to improve recommendations).\n",
        "\n",
        "20. What is the difference between Bagging and Boosting?\n",
        "\n",
        "Bagging: In Bagging, multiple models (typically of the same type, like decision trees) are trained in parallel on different subsets of the data using bootstrapping. The final prediction is made by averaging (regression) or voting (classification) from all models. Boosting: In Boosting, models are trained sequentially, and each new model attempts to correct the errors made by the previous ones. The final prediction is a weighted sum of the predictions from all models, with more weight given to better-performing models. Boosting tends to focus more on hard-to-classify examples.\n",
        "\n",
        "\n",
        "# 21. Train a Bagging Classifier using Decision Trees on a sample dataset and print model accuracy\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Bagging Classifier with Decision Trees\n",
        "# Changed 'base_estimator' to 'estimator'\n",
        "bagging_clf = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate the model\n",
        "y_pred = bagging_clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Bagging Classifier Accuracy: {accuracy:.4f}\")\n",
        "     \n",
        "Bagging Classifier Accuracy: 1.0000\n",
        "\n",
        "# 22. Train a Bagging Regressor using Decision Trees and evaluate using Mean Squared Error (MSE)\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Create a regression dataset\n",
        "X, y = make_regression(n_samples=100, n_features=5, noise=0.1, random_state=42)\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Bagging Regressor with Decision Trees\n",
        "# Changed 'base_estimator' to 'estimator'\n",
        "bagging_reg = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=50, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "bagging_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate the model\n",
        "y_pred = bagging_reg.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Bagging Regressor MSE: {mse:.4f}\")\n",
        "     \n",
        "Bagging Regressor MSE: 5340.8397\n",
        "\n",
        "# 23. Train a Random Forest Classifier on the Breast Cancer dataset and print feature importance scores\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Print feature importance scores\n",
        "importances = rf_clf.feature_importances_\n",
        "print(\"Feature Importance Scores:\")\n",
        "for feature, importance in zip(data.feature_names, importances):\n",
        "    print(f\"{feature}: {importance:.4f}\")\n",
        "\n",
        "     \n",
        "Feature Importance Scores:\n",
        "mean radius: 0.0323\n",
        "mean texture: 0.0111\n",
        "mean perimeter: 0.0601\n",
        "mean area: 0.0538\n",
        "mean smoothness: 0.0062\n",
        "mean compactness: 0.0092\n",
        "mean concavity: 0.0806\n",
        "mean concave points: 0.1419\n",
        "mean symmetry: 0.0033\n",
        "mean fractal dimension: 0.0031\n",
        "radius error: 0.0164\n",
        "texture error: 0.0032\n",
        "perimeter error: 0.0118\n",
        "area error: 0.0295\n",
        "smoothness error: 0.0059\n",
        "compactness error: 0.0046\n",
        "concavity error: 0.0058\n",
        "concave points error: 0.0034\n",
        "symmetry error: 0.0040\n",
        "fractal dimension error: 0.0071\n",
        "worst radius: 0.0780\n",
        "worst texture: 0.0188\n",
        "worst perimeter: 0.0743\n",
        "worst area: 0.1182\n",
        "worst smoothness: 0.0118\n",
        "worst compactness: 0.0175\n",
        "worst concavity: 0.0411\n",
        "worst concave points: 0.1271\n",
        "worst symmetry: 0.0129\n",
        "worst fractal dimension: 0.0069\n",
        "\n",
        "# 24. Train a Random Forest Regressor and compare its performance with a single Decision Tree\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Create a regression dataset\n",
        "X, y = make_regression(n_samples=100, n_features=5, noise=0.1, random_state=42)\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a Random Forest Regressor\n",
        "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_reg.fit(X_train, y_train)\n",
        "\n",
        "# Train a single Decision Tree Regressor\n",
        "tree_reg = DecisionTreeRegressor(random_state=42)\n",
        "tree_reg.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate both models\n",
        "rf_pred = rf_reg.predict(X_test)\n",
        "tree_pred = tree_reg.predict(X_test)\n",
        "\n",
        "rf_mse = mean_squared_error(y_test, rf_pred)\n",
        "tree_mse = mean_squared_error(y_test, tree_pred)\n",
        "\n",
        "print(f\"Random Forest Regressor MSE: {rf_mse:.4f}\")\n",
        "print(f\"Decision Tree Regressor MSE: {tree_mse:.4f}\")\n",
        "\n",
        "     \n",
        "Random Forest Regressor MSE: 5259.6839\n",
        "Decision Tree Regressor MSE: 9910.6505\n",
        "\n",
        "# 25. Compute the Out-of-Bag (OOB) Score for a Random Forest Classifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a Random Forest Classifier with OOB score enabled\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Print the OOB score\n",
        "print(f\"OOB Score: {rf_clf.oob_score_:.4f}\")\n",
        "\n",
        "\n",
        "     \n",
        "OOB Score: 0.9429\n",
        "\n",
        "# 26. Train a Bagging Classifier using SVM as a base estimator and print accuracy\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Bagging Classifier with SVM as the base estimator\n",
        "# Changed 'base_estimator' to 'estimator'\n",
        "bagging_svm = BaggingClassifier(estimator=SVC(), n_estimators=50, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "bagging_svm.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate the model\n",
        "y_pred = bagging_svm.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Bagging Classifier with SVM Accuracy: {accuracy:.4f}\")\n",
        "     \n",
        "Bagging Classifier with SVM Accuracy: 1.0000\n",
        "\n",
        "# 27. Train a Random Forest Classifier with different numbers of trees and compare accuracy\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Random Forests with different numbers of trees and compare accuracy\n",
        "for n_estimators in [10, 50, 100, 200]:\n",
        "    rf_clf = RandomForestClassifier(n_estimators=n_estimators, random_state=42)\n",
        "    rf_clf.fit(X_train, y_train)\n",
        "    y_pred = rf_clf.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Random Forest with {n_estimators} trees Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "     \n",
        "Random Forest with 10 trees Accuracy: 1.0000\n",
        "Random Forest with 50 trees Accuracy: 1.0000\n",
        "Random Forest with 100 trees Accuracy: 1.0000\n",
        "Random Forest with 200 trees Accuracy: 1.0000\n",
        "\n",
        "# 28. Train a Bagging Classifier using Logistic Regression as a base estimator and print AUC score\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Convert y to binary for AUC\n",
        "y_binary = (y == 0).astype(int)\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Bagging Classifier with Logistic Regression\n",
        "# Changed 'base_estimator' to 'estimator'\n",
        "bagging_lr = BaggingClassifier(estimator=LogisticRegression(), n_estimators=50, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "bagging_lr.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities and compute AUC\n",
        "y_pred_prob = bagging_lr.predict_proba(X_test)[:, 1]\n",
        "auc = roc_auc_score(y_test, y_pred_prob)\n",
        "print(f\"Bagging Classifier with Logistic Regression AUC: {auc:.4f}\")\n",
        "     \n",
        "Bagging Classifier with Logistic Regression AUC: 1.0000\n",
        "\n",
        "# 29. Train a Random Forest Regressor and analyze feature importance scores\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Create a regression dataset\n",
        "X, y = make_regression(n_samples=100, n_features=5, noise=0.1, random_state=42)\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a Random Forest Regressor\n",
        "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_reg.fit(X_train, y_train)\n",
        "\n",
        "# Print feature importance scores\n",
        "importances = rf_reg.feature_importances_\n",
        "print(\"Feature Importance Scores:\")\n",
        "for idx, importance in enumerate(importances):\n",
        "    print(f\"Feature {idx}: {importance:.4f}\")\n",
        "\n",
        "     \n",
        "Feature Importance Scores:\n",
        "Feature 0: 0.1923\n",
        "Feature 1: 0.4978\n",
        "Feature 2: 0.0706\n",
        "Feature 3: 0.1815\n",
        "Feature 4: 0.0577\n",
        "\n",
        "# 30. Train an ensemble model using both Bagging and Random Forest and compare accuracy\n",
        "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Bagging with Decision Tree\n",
        "# Changed 'base_estimator' to 'estimator'\n",
        "bagging_clf = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "bagging_pred = bagging_clf.predict(X_test)\n",
        "bagging_accuracy = accuracy_score(y_test, bagging_pred)\n",
        "\n",
        "# Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "rf_pred = rf_clf.predict(X_test)\n",
        "rf_accuracy = accuracy_score(y_test, rf_pred)\n",
        "\n",
        "print(f\"Bagging Classifier Accuracy: {bagging_accuracy:.4f}\")\n",
        "print(f\"Random Forest Classifier Accuracy: {rf_accuracy:.4f}\")\n",
        "     \n",
        "Bagging Classifier Accuracy: 1.0000\n",
        "Random Forest Classifier Accuracy: 1.0000\n",
        "\n",
        "# 31. Train a Random Forest Classifier and tune hyperparameters using GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define the model\n",
        "rf_clf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "# Use GridSearchCV to tune hyperparameters\n",
        "grid_search = GridSearchCV(rf_clf, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters and model performance\n",
        "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "print(f\"Best Accuracy: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "     \n",
        "Best Parameters: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 100}\n",
        "Best Accuracy: 0.9429\n",
        "\n",
        "# 32 Train a Bagging Regressor with different numbers of base estimators and compare performance\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Create a regression dataset\n",
        "X, y = make_regression(n_samples=100, n_features=5, noise=0.1, random_state=42)\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Compare performance with different numbers of base estimators\n",
        "for n_estimators in [10, 50, 100]:\n",
        "    # Changed 'base_estimator' to 'estimator'\n",
        "    bagging_reg = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=n_estimators, random_state=42)\n",
        "    bagging_reg.fit(X_train, y_train)\n",
        "    y_pred = bagging_reg.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    print(f\"Bagging Regressor with {n_estimators} base estimators MSE: {mse:.4f}\")\n",
        "     \n",
        "Bagging Regressor with 10 base estimators MSE: 5697.3880\n",
        "Bagging Regressor with 50 base estimators MSE: 5340.8397\n",
        "Bagging Regressor with 100 base estimators MSE: 5242.4667\n",
        "\n",
        "# 33 Train a Random Forest Classifier and analyze misclassified samples\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train the Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and calculate accuracy\n",
        "y_pred = rf_clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Random Forest Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Find misclassified samples\n",
        "misclassified = X_test[y_pred != y_test]\n",
        "print(f\"Misclassified Samples: {misclassified}\")\n",
        "\n",
        "     \n",
        "Random Forest Accuracy: 1.0000\n",
        "Misclassified Samples: []\n",
        "\n",
        "# 35 Train a Random Forest Classifier and visualize the confusion matrix\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and calculate confusion matrix\n",
        "y_pred = rf_clf.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Display the confusion matrix\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=data.target_names)\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.show()\n",
        "\n",
        "     \n",
        "\n",
        "\n",
        "# 36 Train a Stacking Classifier using Decision Trees, SVM, and Logistic Regression, and compare accuracy\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define base models\n",
        "base_learners = [\n",
        "    ('dt', DecisionTreeClassifier(random_state=42)),\n",
        "    ('svm', SVC(random_state=42)),\n",
        "    ('lr', LogisticRegression(max_iter=200, random_state=42))\n",
        "]\n",
        "\n",
        "# Train Stacking Classifier\n",
        "stacking_clf = StackingClassifier(estimators=base_learners, final_estimator=LogisticRegression())\n",
        "stacking_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate the model\n",
        "y_pred = stacking_clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Stacking Classifier Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "     \n",
        "Stacking Classifier Accuracy: 1.0000\n",
        "\n",
        "# 37 Train a Random Forest Classifier and print the top 5 most important features\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Print the top 5 most important features\n",
        "importances = rf_clf.feature_importances_\n",
        "indices = importances.argsort()[::-1]\n",
        "\n",
        "print(\"Top 5 Most Important Features:\")\n",
        "# Change the loop to iterate only over the length of data.feature_names or importances\n",
        "for i in range(min(5, len(data.feature_names))):\n",
        "    print(f\"{data.feature_names[indices[i]]}: {importances[indices[i]]:.4f}\")\n",
        "     \n",
        "Top 5 Most Important Features:\n",
        "petal width (cm): 0.4340\n",
        "petal length (cm): 0.4173\n",
        "sepal length (cm): 0.1041\n",
        "sepal width (cm): 0.0446\n",
        "\n",
        "# 38 Train a Bagging Classifier and evaluate performance using Precision, Recall, and F1-score\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Convert y to binary for binary classification\n",
        "y_binary = (y == 0).astype(int)\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Bagging Classifier\n",
        "# Changed 'base_estimator' to 'estimator'\n",
        "bagging_clf = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate the model\n",
        "y_pred = bagging_clf.predict(X_test)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-score: {f1:.4f}\")\n",
        "     \n",
        "Precision: 1.0000\n",
        "Recall: 1.0000\n",
        "F1-score: 1.0000\n",
        "\n",
        "# 39 Train a Random Forest Classifier and analyze the effect of max_depth on accuracy\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Random Forest with different max_depth values and compare accuracy\n",
        "for max_depth in [None, 10, 20, 30]:\n",
        "    rf_clf = RandomForestClassifier(n_estimators=100, max_depth=max_depth, random_state=42)\n",
        "    rf_clf.fit(X_train, y_train)\n",
        "    y_pred = rf_clf.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Random Forest with max_depth={max_depth} Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "     \n",
        "Random Forest with max_depth=None Accuracy: 1.0000\n",
        "Random Forest with max_depth=10 Accuracy: 1.0000\n",
        "Random Forest with max_depth=20 Accuracy: 1.0000\n",
        "Random Forest with max_depth=30 Accuracy: 1.0000\n",
        "\n",
        "# 40 Train a Bagging Regressor using different base estimators (DecisionTree and KNeighbors) and compare performance\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Create a regression dataset\n",
        "X, y = make_regression(n_samples=100, n_features=5, noise=0.1, random_state=42)\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Compare performance with different base estimators\n",
        "for base_estimator, name in [(DecisionTreeRegressor(), \"DecisionTreeRegressor\"),\n",
        "                             (KNeighborsRegressor(), \"KNeighborsRegressor\")]:\n",
        "    # Changed 'base_estimator' to 'estimator'\n",
        "    bagging_reg = BaggingRegressor(estimator=base_estimator, n_estimators=50, random_state=42)\n",
        "    bagging_reg.fit(X_train, y_train)\n",
        "    y_pred = bagging_reg.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    print(f\"Bagging Regressor with {name} MSE: {mse:.4f}\")\n",
        "     \n",
        "Bagging Regressor with DecisionTreeRegressor MSE: 5340.8397\n",
        "Bagging Regressor with KNeighborsRegressor MSE: 4585.0029\n",
        "\n",
        "# 41 Train a Random Forest Classifier and evaluate its performance using ROC-AUC Score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Convert labels to binary for ROC-AUC evaluation (Only for binary classification)\n",
        "y_binary = (y == 0).astype(int)  # Just for example, treat one class as \"positive\"\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities and calculate ROC-AUC score\n",
        "y_prob = rf_clf.predict_proba(X_test)[:, 1]  # Probability of the positive class\n",
        "roc_auc = roc_auc_score(y_test, y_prob)\n",
        "print(f\"Random Forest Classifier ROC-AUC Score: {roc_auc:.4f}\")\n",
        "\n",
        "     \n",
        "Random Forest Classifier ROC-AUC Score: 1.0000\n",
        "\n",
        "# 42 Train a Bagging Classifier and evaluate its performance using cross-validation.\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train Bagging Classifier with Decision Tree as the base estimator\n",
        "# Changed 'base_estimator' to 'estimator'\n",
        "bagging_clf = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
        "\n",
        "# Evaluate performance using cross-validation\n",
        "cv_scores = cross_val_score(bagging_clf, X, y, cv=5, scoring='accuracy')\n",
        "print(f\"Bagging Classifier Cross-Validation Accuracy: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
        "     \n",
        "Bagging Classifier Cross-Validation Accuracy: 0.9667 ± 0.0211\n",
        "\n",
        "# 43 Train a Random Forest Classifier and plot the Precision-Recall curve\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Convert labels to binary for Precision-Recall Curve (Only for binary classification)\n",
        "y_binary = (y == 0).astype(int)  # Just for example, treat one class as \"positive\"\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Get predicted probabilities for the positive class\n",
        "y_prob = rf_clf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute precision and recall\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_prob)\n",
        "\n",
        "# Plot the Precision-Recall curve\n",
        "plt.plot(recall, precision, marker='.')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.show()\n",
        "\n",
        "# Calculate and print the AUC (Area Under the Curve)\n",
        "pr_auc = auc(recall, precision)\n",
        "print(f\"Precision-Recall AUC: {pr_auc:.4f}\")\n",
        "\n",
        "\n",
        "     \n",
        "\n",
        "Precision-Recall AUC: 1.0000\n",
        "\n",
        "# 44 Train a Stacking Classifier with Random Forest and Logistic Regression and compare accuracy\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define base models and final estimator\n",
        "base_learners = [\n",
        "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
        "    ('lr', LogisticRegression(max_iter=200, random_state=42))\n",
        "]\n",
        "\n",
        "stacking_clf = StackingClassifier(estimators=base_learners, final_estimator=LogisticRegression())\n",
        "\n",
        "# Train the Stacking Classifier\n",
        "stacking_clf.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate accuracy\n",
        "y_pred = stacking_clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Stacking Classifier Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "     \n",
        "Stacking Classifier Accuracy: 1.0000\n",
        "\n",
        "# 45 Train a Bagging Regressor with different levels of bootstrap samples and compare performance.\n",
        "\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Create a regression dataset\n",
        "X, y = make_regression(n_samples=100, n_features=5, noise=0.1, random_state=42)\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Compare performance with different levels of bootstrap samples\n",
        "for max_samples in [0.5, 0.7, 1.0]:  # 50%, 70%, and 100% of data\n",
        "    # Changed 'base_estimator' to 'estimator'\n",
        "    bagging_reg = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=50,\n",
        "                                  max_samples=max_samples, random_state=42)\n",
        "    bagging_reg.fit(X_train, y_train)\n",
        "    y_pred = bagging_reg.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    print(f\"Bagging Regressor with max_samples={max_samples} MSE: {mse:.4f}\")\n",
        "\n",
        "     \n",
        "Bagging Regressor with max_samples=0.5 MSE: 5571.3286\n",
        "Bagging Regressor with max_samples=0.7 MSE: 5534.3697\n",
        "Bagging Regressor with max_samples=1.0 MSE: 5340.8397\n",
        "\n",
        "\n",
        "     "
      ],
      "metadata": {
        "id": "yHej6Nx8en6q"
      }
    }
  ]
}