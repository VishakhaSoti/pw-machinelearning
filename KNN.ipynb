{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "THEORY QUESTION\n",
        "Q1) What is K-Nearest Neighbors (KNN) and how does it work ?\n",
        "Ans: What is K-Nearest Neighbors (KNN)?\n",
        "\n",
        "K-Nearest Neighbors (KNN) is a supervised machine learning algorithm used for both:\n",
        "\n",
        "Classification (most common)\n",
        "Regression\n",
        "It is instance-based (or lazy learning), meaning it doesn't explicitly learn a model during training — instead, it memorizes the training data and makes predictions only when asked.\n",
        "\n",
        " How Does KNN Work?\n",
        "Choose a value for k (number of neighbors to consider).\n",
        "\n",
        "When a new data point needs a prediction:\n",
        "\n",
        "Measure the distance between this point and all points in the training set (usually Euclidean distance).\n",
        "Select the k closest neighbors (smallest distances).\n",
        "\n",
        "For classification:\n",
        "\n",
        "Majority vote: The most frequent class among the neighbors becomes the predicted class. For regression:\n",
        "Average the values of the neighbors.\n",
        "Return the result.\n",
        "\n",
        "Q2)What is the difference between KNN Classification and KNN Regression ?\n",
        "Ans:KNN Classification predicts a category label (e.g., \"spam\" or \"not spam\") based on the majority class among the K nearest neighbors.\n",
        "\n",
        "KNN Regression predicts a continuous value (e.g., price or temperature) by averaging the values of the K nearest neighbors.\n",
        "\n",
        "Q3)What is the role of the distance metric in KNN ?\n",
        "Ans:The distance metric in KNN determines how \"close\" or \"similar\" data points are. It plays a key role in selecting the K nearest neighbors, which directly affects the model's prediction accuracy. Common metrics include Euclidean, Manhattan, and Minkowski distances.\n",
        "\n",
        "Q4)What is the Curse of Dimensionality in KNN ?\n",
        "Ans:The Curse of Dimensionality in KNN refers to the problem that arises when data has too many features (high dimensions). In high-dimensional spaces:\n",
        "\n",
        "Distances between points become less meaningful — all points tend to look equally far from each other.\n",
        "It becomes harder to find truly \"nearest\" neighbors, reducing the effectiveness and accuracy of KNN.\n",
        "This can lead to poor performance and overfitting.\n",
        "\n",
        "Q5)How can we choose the best value of K in KNN ?\n",
        "Ans:To choose the best value of K in KNN:\n",
        "\n",
        "Use Cross-Validation: Test different K values using techniques like k-fold cross-validation to find the one with the best performance on validation data.\n",
        "\n",
        "Plot Accuracy vs. K: Plot model accuracy (or error rate) against various K values and choose the one where accuracy is highest and stable.\n",
        "\n",
        "Avoid Extreme Values:\n",
        "\n",
        "K too small (e.g., 1) → model may overfit and be sensitive to noise.\n",
        "K too large → model may underfit and ignore local patterns.\n",
        "Typical starting point: Try odd values between 1 and √N (N = number of samples).\n",
        "\n",
        "Q6)What are KD Tree and Ball Tree in KNN ?\n",
        "Ans:KD Tree and Ball Tree are data structures used to speed up nearest neighbor search in KNN, especially with large datasets.\n",
        "\n",
        "KD Tree (K-Dimensional Tree):\n",
        "\n",
        "Divides data along axes (features) at median values.\n",
        "Works well for low-dimensional data.\n",
        "Becomes inefficient in high dimensions due to the curse of dimensionality.\n",
        "Ball Tree:\n",
        "\n",
        "Divides data into nested hyperspheres (\"balls\").\n",
        "Handles high-dimensional data better than KD Tree.\n",
        "More flexible with non-axis-aligned splits.\n",
        "Both structures reduce the number of distance calculations during KNN prediction.\n",
        "\n",
        "Q7)When should you use KD Tree vs. Ball Tree ?\n",
        "Ans:Use KD Tree when:\n",
        "\n",
        "The data has low dimensionality (typically < 20 features).\n",
        "Speed and simplicity are important.\n",
        "You want fast queries on structured data.\n",
        "Use Ball Tree when:\n",
        "\n",
        "The data has high dimensionality (typically ≥ 20 features).\n",
        "The data is not well aligned with axes (more irregular or clustered).\n",
        "KD Tree becomes slow or ineffective due to the curse of dimensionality.\n",
        "Summary:\n",
        "\n",
        "Low dimensions → KD Tree\n",
        "High dimensions or complex structure → Ball Tree\n",
        "Q8)What are the disadvantages of KNN ?\n",
        "Ans:Disadvantages of KNN:\n",
        "\n",
        "Slow Prediction Time: Needs to compute distances to all training points at prediction time.\n",
        "Sensitive to Irrelevant Features: Unimportant features can distort distance calculations.\n",
        "Affected by the Curse of Dimensionality: Performance drops in high-dimensional spaces.\n",
        "Requires Feature Scaling: Distance metrics like Euclidean need normalized features.\n",
        "Memory Intensive: Stores the entire training dataset.\n",
        "Sensitive to Noise and Outliers: Noisy data can mislead neighbor voting.\n",
        "Q9)How does feature scaling affect KNN ?\n",
        "Ans:Feature scaling is crucial for KNN because it ensures all features contribute equally to the distance calculation. Without scaling, features with larger numeric ranges dominate the distance metric, which can distort neighbor selection and reduce accuracy. Scaling (e.g., normalization or standardization) puts features on a similar scale, improving KNN’s performance.\n",
        "\n",
        "Q10)What is PCA (Principal Component Analysis) ?\n",
        "Ans:PCA (Principal Component Analysis) is a dimensionality reduction technique that transforms high-dimensional data into a lower-dimensional space by identifying the directions (principal components) where the data varies the most. It helps to:\n",
        "\n",
        "Reduce the number of features while preserving most of the data’s variance.\n",
        "Remove redundant or correlated features.\n",
        "Simplify data visualization and improve efficiency for machine learning models.\n",
        "Q11) How does PCA work ?\n",
        "Ans:Sure! Here’s a clear explanation of how PCA (Principal Component Analysis) works:\n",
        "\n",
        "What is PCA?\n",
        "PCA is a technique used in statistics and machine learning to reduce the dimensionality of a dataset while preserving as much variability (information) as possible. It transforms the original features into a new set of features called principal components, which are uncorrelated and ordered by the amount of variance they explain in the data.\n",
        "\n",
        "How PCA works (step-by-step):\n",
        "Start with a dataset Suppose you have a dataset with many correlated features (dimensions).\n",
        "\n",
        "Standardize the data PCA works best if each feature has zero mean and unit variance. So, subtract the mean and divide by the standard deviation for each feature.\n",
        "\n",
        "Compute the covariance matrix This matrix shows how features vary together. For example, the covariance between feature A and B tells you if they increase or decrease together.\n",
        "\n",
        "Calculate eigenvectors and eigenvalues of the covariance matrix\n",
        "\n",
        "Eigenvectors represent directions in the feature space (these will be your principal components).\n",
        "Eigenvalues represent the amount of variance in the data along these eigenvectors.\n",
        "Sort eigenvectors by eigenvalues Order the eigenvectors from largest eigenvalue to smallest. The eigenvector with the highest eigenvalue points to the direction with the greatest variance.\n",
        "\n",
        "Choose the top k eigenvectors Select the first k eigenvectors (principal components) that explain the most variance. This reduces the dimensionality to k dimensions.\n",
        "\n",
        "Transform the original data Project the original data onto the new k principal components, giving you a new representation of the data with fewer features but maximum variance preserved.\n",
        "\n",
        "Q12)What is the geometric intuition behind PCA ?\n",
        "Ans:Great question! The geometric intuition behind PCA helps make the math feel more natural and visual.\n",
        "\n",
        "Geometric Intuition of PCA\n",
        "Imagine you have a cloud of data points scattered in a high-dimensional space (for simplicity, think 2D or 3D).\n",
        "\n",
        "Data as a point cloud Each data point is a vector in this space. The shape of this cloud reflects how the features vary together.\n",
        "\n",
        "Goal: Find new coordinate axes that best describe the data Instead of the original axes (which might not capture the main directions of variation well), PCA finds new axes aligned with the directions where the data varies the most.\n",
        "\n",
        "First principal component (PC1): Direction of maximum variance\n",
        "\n",
        "PCA tries to find a line (a 1D axis) through the data cloud that captures the greatest spread of points.\n",
        "Think of fitting a line through the cloud so that if you project all points onto this line, their projected points spread out the most.\n",
        "Second principal component (PC2): Direction of next maximum variance, orthogonal to PC1\n",
        "\n",
        "Then PCA finds another line perpendicular to the first one that captures the next highest variance.\n",
        "This is like finding the second-best axis along which points are spread out, but this axis must be at a right angle to the first.\n",
        "Higher dimensions\n",
        "\n",
        "In higher dimensions, PCA finds successive orthogonal axes capturing decreasing amounts of variance.\n",
        "Dimensionality reduction\n",
        "\n",
        "By keeping only the first few principal components, you capture most of the data’s spread while reducing the number of dimensions.\n",
        "Geometrically, this means you approximate the original high-dimensional cloud by its projection onto a lower-dimensional subspace defined by those principal components.\n",
        "Visual analogy\n",
        "Imagine a flat, elongated cloud of points lying roughly along some diagonal in a 3D room.\n",
        "PCA finds that diagonal line as PC1 because that's where the data stretches the most.\n",
        "Then it finds a perpendicular line (PC2) where the points still spread but less so.\n",
        "PC3 would be perpendicular to both but might have very little spread (variance).\n",
        "By projecting onto just PC1 and PC2, you reduce from 3D to 2D while preserving most of the data’s structure.\n",
        "\n",
        "Q13)What is the difference between Feature Selection and Feature Extraction ?\n",
        "Ans:Great question! Feature Selection and Feature Extraction are both techniques used in the preprocessing step of machine learning to reduce the number of input variables, but they work quite differently:\n",
        "\n",
        "Feature Selection\n",
        "What it does: Selects a subset of the original features from the dataset.\n",
        "\n",
        "Goal: Choose the most relevant features without changing them.\n",
        "\n",
        "How: Removes irrelevant, redundant, or noisy features, keeping only those that contribute the most to the model.\n",
        "\n",
        "Example methods:\n",
        "\n",
        "Filter methods (e.g., correlation, chi-square test)\n",
        "Wrapper methods (e.g., recursive feature elimination)\n",
        "Embedded methods (e.g., Lasso regularization)\n",
        "Result: A reduced set of the original features, with the same meaning and interpretation.\n",
        "\n",
        "Feature Extraction\n",
        "What it does: Creates new features by transforming or combining the original features.\n",
        "\n",
        "Goal: Transform data into a new feature space that is often lower-dimensional but preserves the most important information.\n",
        "\n",
        "How: Combines and compresses features to capture the essence of the data.\n",
        "\n",
        "Example methods:\n",
        "\n",
        "Principal Component Analysis (PCA)\n",
        "Linear Discriminant Analysis (LDA)\n",
        "Autoencoders (in deep learning)\n",
        "Result: A new set of features (components or embeddings), which may be hard to interpret directly but often improve model performance.\n",
        "\n",
        "Quick summary\n",
        "Aspect\tFeature Selection\tFeature Extraction\n",
        "Output features\tSubset of original features\tNew transformed features\n",
        "Interpretation\tEasy, original meaning retained\tHarder, features are combinations\n",
        "Purpose\tRemove irrelevant features\tReduce dimensionality with transformation\n",
        "Example\tSelect top 10 features\tUse PCA to get 5 principal components\n",
        "Q14)What are Eigenvalues and Eigenvectors in PCA ?\n",
        "Ans:Great follow-up! Eigenvalues and eigenvectors are key concepts in Principal Component Analysis (PCA). Here's a clear explanation:\n",
        "\n",
        "In PCA:\n",
        "You start with a dataset and compute its covariance matrix (or correlation matrix). PCA tries to find new axes (directions) to represent the data, where these axes maximize the variance.\n",
        "\n",
        "Eigenvectors\n",
        "What they are: The directions (vectors) along which the data varies the most.\n",
        "Role in PCA: Each eigenvector represents a principal component, i.e., a new axis or dimension in the transformed feature space.\n",
        "Interpretation: The eigenvectors define the directions of the new coordinate system.\n",
        "Eigenvalues\n",
        "What they are: Scalars associated with each eigenvector that tell you how much variance the data has along that eigenvector.\n",
        "Role in PCA: Eigenvalues indicate the importance or strength of each principal component.\n",
        "Interpretation: The larger the eigenvalue, the more variance that component explains.\n",
        "Summary:\n",
        "Eigenvectors = directions of the principal components.\n",
        "Eigenvalues = amount of variance explained by those components.\n",
        "How PCA uses them:\n",
        "Calculate the covariance matrix of your data.\n",
        "Compute the eigenvectors and eigenvalues of this matrix.\n",
        "Sort eigenvectors by descending eigenvalues.\n",
        "Pick the top k eigenvectors to form your principal components.\n",
        "Transform your original data onto this new reduced-dimensional space.\n",
        "Q15)How do you decide the number of components to keep in PCA ?\n",
        "Ans:In short: Choose the number of components that explain a sufficient amount of total variance, typically 90-95%.\n",
        "\n",
        "This is usually done by looking at the cumulative explained variance ratio and selecting the smallest number of components whose cumulative variance meets your threshold. Another common method is using a scree plot to find the “elbow” point where adding more components yields diminishing returns.\n",
        "\n",
        "Q16)Can PCA be used for classification ?\n",
        "Ans:PCA itself is not a classification algorithm, but it can be used as a preprocessing step before classification.\n",
        "\n",
        "How it helps: PCA reduces the dimensionality of the data, removing noise and redundancy, which can improve the performance and speed of classifiers.\n",
        "After PCA transforms the data into principal components, you feed these components into a classification algorithm (like SVM, logistic regression, etc.).\n",
        "So, PCA doesn’t classify but helps classifiers by providing better input features.\n",
        "\n",
        "Q17)What are the limitations of PCA ?\n",
        "Ans:Here are some key limitations of PCA:\n",
        "\n",
        "Linearity: PCA only captures linear relationships between features. It can’t effectively reduce dimensionality if the data lies on a nonlinear manifold.\n",
        "\n",
        "Interpretability: The new features (principal components) are linear combinations of original features and often lack clear, intuitive meaning.\n",
        "\n",
        "Sensitivity to Scaling: PCA is sensitive to the scale of variables, so features need to be standardized before applying it.\n",
        "\n",
        "Variance Focused: PCA maximizes variance, which doesn’t always correspond to the most discriminative features for tasks like classification.\n",
        "\n",
        "Noise Sensitivity: If the data contains a lot of noise, PCA may capture noise in its principal components, especially if the noise variance is high.\n",
        "\n",
        "Not Robust to Outliers: Outliers can heavily influence the principal components.\n",
        "\n",
        "Q18)How do KNN and PCA complement each other ?\n",
        "Ans:Great question! KNN (K-Nearest Neighbors) and PCA can complement each other very well:\n",
        "\n",
        "PCA reduces dimensionality by transforming the data into fewer, more informative features.\n",
        "This simplifies the feature space for KNN, which relies on distance calculations.\n",
        "By reducing dimensions, PCA helps mitigate the “curse of dimensionality” that can degrade KNN’s performance.\n",
        "PCA can also remove noise and redundant features, improving KNN’s accuracy and speed.\n",
        "In short: PCA preprocesses the data to make KNN more efficient and effective! Want me to explain how to combine them in a workflow?\n",
        "\n",
        "Q19)How does KNN handle missing values in a dataset ?\n",
        "Ans:KNN itself does not natively handle missing values—it requires a complete dataset to compute distances.\n",
        "\n",
        "However, there are common strategies to handle missing data before or during KNN:\n",
        "\n",
        "Imputation before KNN:\n",
        "\n",
        "Fill missing values using methods like mean/median imputation, or more advanced techniques (e.g., KNN imputation itself).\n",
        "Then run KNN on the completed dataset.\n",
        "KNN Imputation:\n",
        "\n",
        "Use KNN to impute missing values by finding the nearest neighbors based on available features and averaging their values for the missing entries.\n",
        "Ignore samples or features:\n",
        "\n",
        "Drop rows or columns with too many missing values (if acceptable).\n",
        "Q20) What are the key differences between PCA and Linear Discriminant Analysis (LDA)?\n",
        "Ans:Great comparison! Here are the key differences between PCA and LDA:\n",
        "\n",
        "Aspect\tPCA\tLDA\n",
        "Type of technique\tUnsupervised dimensionality reduction\tSupervised dimensionality reduction\n",
        "Goal\tMaximize variance in the data\tMaximize class separability (discrimination)\n",
        "Uses labels?\tNo\tYes, uses class labels\n",
        "Focus\tCaptures directions with highest overall variance\tFinds directions that best separate classes\n",
        "Output features\tPrincipal components\tLinear discriminants\n",
        "Number of components\tUp to number of original features\tUp to (number of classes - 1)\n",
        "When to use\tWhen no labels or to explore data structure\tWhen you want to improve classification performance\n",
        "PRATICAL QUESTION\n",
        "Q21)Train a KNN Classifier on the Iris dataset and print model accuracy.\n",
        "Ans:\n",
        "\n",
        "\n",
        "[ ]\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split data into train and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize KNN classifier (e.g., with k=5)\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "# Train the model\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = knn.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "Model Accuracy: 1.00\n",
        "Q22)Train a KNN Regressor on a synthetic dataset and evaluate using Mean Squared Error (MSE).\n",
        "Ans:\n",
        "\n",
        "\n",
        "[ ]\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generate synthetic regression dataset\n",
        "X, y = make_regression(n_samples=200, n_features=5, noise=10, random_state=42)\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize KNN Regressor (e.g., k=5)\n",
        "knn_regressor = KNeighborsRegressor(n_neighbors=5)\n",
        "\n",
        "# Train the model\n",
        "knn_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = knn_regressor.predict(X_test)\n",
        "\n",
        "# Evaluate using Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
        "\n",
        "Mean Squared Error (MSE): 1609.75\n",
        "Q23)Train a KNN Classifier using different distance metrics (Euclidean and Manhattan) and compare accuracy.\n",
        "Ans:\n",
        "\n",
        "\n",
        "[ ]\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# KNN with Euclidean distance (default)\n",
        "knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
        "knn_euclidean.fit(X_train, y_train)\n",
        "y_pred_euclidean = knn_euclidean.predict(X_test)\n",
        "acc_euclidean = accuracy_score(y_test, y_pred_euclidean)\n",
        "\n",
        "# KNN with Manhattan distance\n",
        "knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='manhattan')\n",
        "knn_manhattan.fit(X_train, y_train)\n",
        "y_pred_manhattan = knn_manhattan.predict(X_test)\n",
        "acc_manhattan = accuracy_score(y_test, y_pred_manhattan)\n",
        "\n",
        "print(f\"Accuracy with Euclidean distance: {acc_euclidean:.2f}\")\n",
        "print(f\"Accuracy with Manhattan distance: {acc_manhattan:.2f}\")\n",
        "\n",
        "Accuracy with Euclidean distance: 1.00\n",
        "Accuracy with Manhattan distance: 1.00\n",
        "Q24)Train a KNN Classifier with different values of K and visualize decision boundaries\n",
        "Ans:\n",
        "\n",
        "\n",
        "[ ]\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data[:, :2]  # Use only first two features for 2D plot\n",
        "y = iris.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create color maps\n",
        "cmap_light = plt.cm.Pastel2\n",
        "cmap_bold = plt.cm.Set1\n",
        "\n",
        "# Define the range for plotting\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
        "                     np.arange(y_min, y_max, 0.02))\n",
        "\n",
        "# Different values of K to try\n",
        "k_values = [1, 5, 15]\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "for i, k in enumerate(k_values, 1):\n",
        "    # Train KNN classifier\n",
        "    knn = KNeighborsClassifier(n_neighbors=k)\n",
        "    knn.fit(X_train, y_train)\n",
        "\n",
        "    # Predict on mesh grid\n",
        "    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    # Plot decision boundary\n",
        "    plt.subplot(1, len(k_values), i)\n",
        "    plt.contourf(xx, yy, Z, cmap=cmap_light)\n",
        "\n",
        "    # Plot training points\n",
        "    plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cmap_bold, edgecolor='k', s=50)\n",
        "\n",
        "    plt.title(f\"KNN (k={k})\")\n",
        "    plt.xlabel(iris.feature_names[0])\n",
        "    plt.ylabel(iris.feature_names[1])\n",
        "    plt.xlim(xx.min(), xx.max())\n",
        "    plt.ylim(yy.min(), yy.max())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "Q25)Apply Feature Scaling before training a KNN model and compare results with unscaled data.\n",
        "Ans:\n",
        "\n",
        "\n",
        "[ ]\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# --- Without scaling ---\n",
        "knn_unscaled = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_unscaled.fit(X_train, y_train)\n",
        "y_pred_unscaled = knn_unscaled.predict(X_test)\n",
        "acc_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
        "\n",
        "# --- With feature scaling ---\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = knn_scaled.predict(X_test_scaled)\n",
        "acc_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "print(f\"Accuracy without scaling: {acc_unscaled:.2f}\")\n",
        "print(f\"Accuracy with scaling: {acc_scaled:.2f}\")\n",
        "\n",
        "Accuracy without scaling: 1.00\n",
        "Accuracy with scaling: 1.00\n",
        "Q26)Train a PCA model on synthetic data and print the explained variance ratio for each component.\n",
        "Ans:\n",
        "\n",
        "\n",
        "[ ]\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Generate synthetic dataset\n",
        "X, _ = make_classification(n_samples=200, n_features=10, n_informative=5, random_state=42)\n",
        "\n",
        "# Train PCA\n",
        "pca = PCA()\n",
        "pca.fit(X)\n",
        "\n",
        "# Print explained variance ratio for each component\n",
        "print(\"Explained variance ratio of each principal component:\")\n",
        "for i, ratio in enumerate(pca.explained_variance_ratio_, 1):\n",
        "    print(f\"Component {i}: {ratio:.4f}\")\n",
        "\n",
        "Explained variance ratio of each principal component:\n",
        "Component 1: 0.3166\n",
        "Component 2: 0.2228\n",
        "Component 3: 0.1627\n",
        "Component 4: 0.1107\n",
        "Component 5: 0.0721\n",
        "Component 6: 0.0462\n",
        "Component 7: 0.0360\n",
        "Component 8: 0.0329\n",
        "Component 9: 0.0000\n",
        "Component 10: 0.0000\n",
        "Q27)Apply PCA before training a KNN Classifier and compare accuracy with and without PCA.\n",
        "Ans:\n",
        "\n",
        "\n",
        "[ ]\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load data\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature scaling is important before PCA and KNN\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# --- KNN without PCA ---\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train_scaled, y_train)\n",
        "y_pred = knn.predict(X_test_scaled)\n",
        "acc_without_pca = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# --- Apply PCA ---\n",
        "pca = PCA(n_components=2)  # reduce to 2 components for illustration\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "# --- KNN with PCA ---\n",
        "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_pca.fit(X_train_pca, y_train)\n",
        "y_pred_pca = knn_pca.predict(X_test_pca)\n",
        "acc_with_pca = accuracy_score(y_test, y_pred_pca)\n",
        "\n",
        "print(f\"Accuracy without PCA: {acc_without_pca:.2f}\")\n",
        "print(f\"Accuracy with PCA (2 components): {acc_with_pca:.2f}\")\n",
        "\n",
        "Accuracy without PCA: 1.00\n",
        "Accuracy with PCA (2 components): 0.93\n",
        "Q28)Perform Hyperparameter Tuning on a KNN Classifier using GridSearchCV.\n",
        "Ans:\n",
        "\n",
        "\n",
        "[ ]\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load data\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define KNN model\n",
        "knn = KNeighborsClassifier()\n",
        "\n",
        "# Define parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'n_neighbors': [3, 5, 7, 9],\n",
        "    'metric': ['euclidean', 'manhattan', 'minkowski']\n",
        "}\n",
        "\n",
        "# Setup GridSearchCV\n",
        "grid_search = GridSearchCV(knn, param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Run grid search on training data\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Best parameters\n",
        "print(\"Best parameters:\", grid_search.best_params_)\n",
        "\n",
        "# Evaluate best model on test data\n",
        "best_knn = grid_search.best_estimator_\n",
        "test_accuracy = best_knn.score(X_test_scaled, y_test)\n",
        "print(f\"Test set accuracy with best parameters: {test_accuracy:.2f}\")\n",
        "\n",
        "Best parameters: {'metric': 'manhattan', 'n_neighbors': 9}\n",
        "Test set accuracy with best parameters: 1.00\n",
        "Q29)Train a KNN Classifier and check the number of misclassified samples.\n",
        "Ans:\n",
        "\n",
        "\n",
        "[ ]\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Load data\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train KNN\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = knn.predict(X_test)\n",
        "\n",
        "# Calculate number of misclassified samples\n",
        "num_misclassified = (y_test != y_pred).sum()\n",
        "print(f\"Number of misclassified samples: {num_misclassified}\")\n",
        "\n",
        "Number of misclassified samples: 0\n",
        "Q30)Train a PCA model and visualize the cumulative explained variance.\n",
        "Ans:\n",
        "\n",
        "\n",
        "[ ]\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load dataset (Iris for example)\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "\n",
        "# Scale features before PCA\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train PCA\n",
        "pca = PCA()\n",
        "pca.fit(X_scaled)\n",
        "\n",
        "# Calculate cumulative explained variance\n",
        "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
        "\n",
        "# Plot cumulative explained variance\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', linestyle='--', color='b')\n",
        "plt.title('Cumulative Explained Variance by PCA Components')\n",
        "plt.xlabel('Number of Principal Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "Q31)Train a KNN Classifier using different values of the weights parameter (uniform vs. distance) and compare accuracy.\n",
        "Ans:\n",
        "\n",
        "\n",
        "[ ]\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# KNN with uniform weights (all neighbors weighted equally)\n",
        "knn_uniform = KNeighborsClassifier(n_neighbors=5, weights='uniform')\n",
        "knn_uniform.fit(X_train_scaled, y_train)\n",
        "y_pred_uniform = knn_uniform.predict(X_test_scaled)\n",
        "acc_uniform = accuracy_score(y_test, y_pred_uniform)\n",
        "\n",
        "# KNN with distance weights (closer neighbors weighted more)\n",
        "knn_distance = KNeighborsClassifier(n_neighbors=5, weights='distance')\n",
        "knn_distance.fit(X_train_scaled, y_train)\n",
        "y_pred_distance = knn_distance.predict(X_test_scaled)\n",
        "acc_distance = accuracy_score(y_test, y_pred_distance)\n",
        "\n",
        "print(f\"Accuracy with uniform weights: {acc_uniform:.2f}\")\n",
        "print(f\"Accuracy with distance weights: {acc_distance:.2f}\")\n",
        "\n",
        "Accuracy with uniform weights: 1.00\n",
        "Accuracy with distance weights: 1.00\n",
        "Q32)Train a KNN Regressor and analyze the effect of different K values on performance\n",
        "Ans:\n",
        "\n",
        "\n",
        "[ ]\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generate synthetic regression dataset\n",
        "X, y = make_regression(n_samples=300, n_features=5, noise=15, random_state=42)\n",
        "\n",
        "# Split dataset into training and testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# List to store MSE for different K values\n",
        "mse_scores = []\n",
        "\n",
        "# Try K values from 1 to 20\n",
        "k_values = range(1, 21)\n",
        "\n",
        "for k in k_values:\n",
        "    # Initialize KNN regressor with current k\n",
        "    knn = KNeighborsRegressor(n_neighbors=k)\n",
        "    knn.fit(X_train, y_train)\n",
        "\n",
        "    # Predict on test set\n",
        "    y_pred = knn.predict(X_test)\n",
        "\n",
        "    # Compute MSE and append to list\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    mse_scores.append(mse)\n",
        "\n",
        "# Plotting MSE vs K\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(k_values, mse_scores, marker='o')\n",
        "plt.title('KNN Regression: Effect of K on Mean Squared Error')\n",
        "plt.xlabel('Number of Neighbors (K)')\n",
        "plt.ylabel('Mean Squared Error (MSE)')\n",
        "plt.xticks(k_values)\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "Q33)Implement KNN Imputation for handling missing values in a dataset.\n",
        "Ans:\n",
        "\n",
        "\n",
        "[ ]\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.impute import KNNImputer\n",
        "\n",
        "# Create a sample dataset with missing values (NaNs)\n",
        "data = {\n",
        "    'Feature1': [1.0, 2.0, np.nan, 4.0, 5.0],\n",
        "    'Feature2': [5.0, np.nan, np.nan, 8.0, 10.0],\n",
        "    'Feature3': [10.0, 9.0, 8.0, np.nan, 6.0]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "print(\"Original Data with Missing Values:\")\n",
        "print(df)\n",
        "\n",
        "# Initialize KNNImputer (default n_neighbors=5)\n",
        "imputer = KNNImputer(n_neighbors=2)\n",
        "\n",
        "# Apply KNN imputation\n",
        "df_imputed = imputer.fit_transform(df)\n",
        "\n",
        "# Convert back to DataFrame for readability\n",
        "df_imputed = pd.DataFrame(df_imputed, columns=df.columns)\n",
        "print(\"\\nData after KNN Imputation:\")\n",
        "print(df_imputed)\n",
        "\n",
        "Original Data with Missing Values:\n",
        "   Feature1  Feature2  Feature3\n",
        "0       1.0       5.0      10.0\n",
        "1       2.0       NaN       9.0\n",
        "2       NaN       NaN       8.0\n",
        "3       4.0       8.0       NaN\n",
        "4       5.0      10.0       6.0\n",
        "\n",
        "Data after KNN Imputation:\n",
        "   Feature1  Feature2  Feature3\n",
        "0       1.0       5.0      10.0\n",
        "1       2.0       6.5       9.0\n",
        "2       3.5       7.5       8.0\n",
        "3       4.0       8.0       7.5\n",
        "4       5.0      10.0       6.0\n",
        "Q35)Train a PCA model and visualize the data projection onto the first two principal components.\n",
        "Ans:\n",
        "\n",
        "\n",
        "[ ]\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "target_names = iris.target_names\n",
        "\n",
        "# Scale data before PCA\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train PCA to reduce to 2 components\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Plot the projection\n",
        "plt.figure(figsize=(8,6))\n",
        "colors = ['r', 'g', 'b']\n",
        "\n",
        "for color, target, target_name in zip(colors, [0, 1, 2], target_names):\n",
        "    plt.scatter(X_pca[y == target, 0], X_pca[y == target, 1],\n",
        "                color=color, lw=2, label=target_name)\n",
        "\n",
        "plt.xlabel('First Principal Component')\n",
        "plt.ylabel('Second Principal Component')\n",
        "plt.title('PCA: Projection onto first two principal components')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "Q35)Train a KNN Classifier using the KD Tree and Ball Tree algorithms and compare performance.\n",
        "Ans:\n",
        "\n",
        "\n",
        "[ ]\n",
        "import time\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "results = {}\n",
        "\n",
        "for algo in ['kd_tree', 'ball_tree']:\n",
        "    knn = KNeighborsClassifier(n_neighbors=5, algorithm=algo)\n",
        "\n",
        "    start_time = time.time()\n",
        "    knn.fit(X_train_scaled, y_train)\n",
        "    train_time = time.time() - start_time\n",
        "\n",
        "    y_pred = knn.predict(X_test_scaled)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    results[algo] = {'accuracy': acc, 'train_time_sec': train_time}\n",
        "\n",
        "# Print results\n",
        "for algo, metrics in results.items():\n",
        "    print(f\"Algorithm: {algo}\")\n",
        "    print(f\" - Accuracy: {metrics['accuracy']:.4f}\")\n",
        "    print(f\" - Training time: {metrics['train_time_sec']:.6f} seconds\\n\")\n",
        "\n",
        "Algorithm: kd_tree\n",
        " - Accuracy: 1.0000\n",
        " - Training time: 0.001322 seconds\n",
        "\n",
        "Algorithm: ball_tree\n",
        " - Accuracy: 1.0000\n",
        " - Training time: 0.001569 seconds\n",
        "\n",
        "Q36)Train a PCA model on a high-dimensional dataset and visualize the Scree plot.\n",
        "Ans:\n",
        "\n",
        "\n",
        "[ ]\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Generate a high-dimensional synthetic dataset (e.g., 100 features)\n",
        "X, _ = make_classification(n_samples=300, n_features=100, n_informative=20, random_state=42)\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train PCA without reducing components (to keep all)\n",
        "pca = PCA()\n",
        "pca.fit(X_scaled)\n",
        "\n",
        "# Explained variance ratio\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "\n",
        "# Scree plot\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.bar(range(1, len(explained_variance) + 1), explained_variance, alpha=0.7, color='blue')\n",
        "plt.xlabel('Principal Component')\n",
        "plt.ylabel('Explained Variance Ratio')\n",
        "plt.title('Scree Plot')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "Q37)Train a KNN Classifier and evaluate performance using Precision, Recall, and F1-Score.\n",
        "Ans:\n",
        "\n",
        "\n",
        "[ ]\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train KNN classifier\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = knn.predict(X_test_scaled)\n",
        "\n",
        "# Print classification report (includes Precision, Recall, F1-Score)\n",
        "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n",
        "\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "      setosa       1.00      1.00      1.00        10\n",
        "  versicolor       1.00      1.00      1.00         9\n",
        "   virginica       1.00      1.00      1.00        11\n",
        "\n",
        "    accuracy                           1.00        30\n",
        "   macro avg       1.00      1.00      1.00        30\n",
        "weighted avg       1.00      1.00      1.00        30\n",
        "\n",
        "Q38)Train a PCA model and analyze the effect of different numbers of components on accuracy.\n",
        "Ans:\n",
        "\n",
        "\n",
        "[ ]\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load data\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Range of PCA components to test\n",
        "n_components = range(1, X.shape[1] + 1)\n",
        "accuracies = []\n",
        "\n",
        "for n in n_components:\n",
        "    # Apply PCA with n components\n",
        "    pca = PCA(n_components=n)\n",
        "    X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "    X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "    # Train KNN classifier on PCA-reduced data\n",
        "    knn = KNeighborsClassifier(n_neighbors=5)\n",
        "    knn.fit(X_train_pca, y_train)\n",
        "\n",
        "    # Predict and evaluate accuracy\n",
        "    y_pred = knn.predict(X_test_pca)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    accuracies.append(acc)\n",
        "\n",
        "# Plot accuracy vs number of PCA components\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(n_components, accuracies, marker='o')\n",
        "plt.xlabel('Number of PCA Components')\n",
        "plt.ylabel('Test Set Accuracy')\n",
        "plt.title('Effect of Number of PCA Components on KNN Accuracy')\n",
        "plt.xticks(n_components)\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "Q39)Train a KNN Classifier with different leaf_size values and compare accuracy.\n",
        "Ans:\n",
        "\n",
        "\n",
        "[ ]\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Test different leaf_size values\n",
        "leaf_sizes = range(1, 51, 5)  # From 1 to 50, step 5\n",
        "accuracies = []\n",
        "\n",
        "for leaf_size in leaf_sizes:\n",
        "    knn = KNeighborsClassifier(n_neighbors=5, algorithm='kd_tree', leaf_size=leaf_size)\n",
        "    knn.fit(X_train_scaled, y_train)\n",
        "    y_pred = knn.predict(X_test_scaled)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    accuracies.append(acc)\n",
        "\n",
        "# Plot results\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(leaf_sizes, accuracies, marker='o')\n",
        "plt.title('Effect of leaf_size on KNN Classifier Accuracy')\n",
        "plt.xlabel('leaf_size')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "Q40)Train a PCA model and visualize how data points are transformed before and after PCA.\n",
        "Ans:\n",
        "\n",
        "\n",
        "[ ]\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "target_names = iris.target_names\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply PCA (2 components)\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Function to plot data points with labels\n",
        "def plot_data(X, y, title, target_names):\n",
        "    colors = ['r', 'g', 'b']\n",
        "    plt.figure(figsize=(8,6))\n",
        "    for color, i, target_name in zip(colors, np.unique(y), target_names):\n",
        "        plt.scatter(X[y == i, 0], X[y == i, 1], alpha=0.7, color=color, label=target_name)\n",
        "    plt.legend()\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Feature 1')\n",
        "    plt.ylabel('Feature 2')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Plot first two original features (before PCA)\n",
        "plot_data(X_scaled[:, :2], y, 'Original Data (First Two Features)', target_names)\n",
        "\n",
        "# Plot data after PCA\n",
        "plot_data(X_pca, y, 'Data after PCA (First Two Principal Components)', target_names)\n",
        "\n",
        "\n",
        "Q41)Train a KNN Classifier on a real-world dataset (Wine dataset) and print classification report.\n",
        "Ans:\n",
        "\n",
        "\n",
        "[ ]\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load the dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train the KNN Classifier\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = knn.predict(X_test_scaled)\n",
        "\n",
        "# Print the classification report\n",
        "print(\"Classification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred, target_names=wine.target_names))\n",
        "\n",
        "Classification Report:\n",
        "\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "     class_0       0.95      1.00      0.97        19\n",
        "     class_1       1.00      0.90      0.95        21\n",
        "     class_2       0.93      1.00      0.97        14\n",
        "\n",
        "    accuracy                           0.96        54\n",
        "   macro avg       0.96      0.97      0.96        54\n",
        "weighted avg       0.97      0.96      0.96        54\n",
        "\n",
        "Q42)Train a KNN Regressor and analyze the effect of different distance metrics on prediction error.\n",
        "Ans:\n",
        "\n",
        "\n",
        "[ ]\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target  # Note: y is originally for classification\n",
        "\n",
        "# For regression, let's pretend y is continuous (for analysis purpose)\n",
        "y = y.astype(float)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Distance metrics to test\n",
        "metrics = {\n",
        "    'euclidean': {'metric': 'minkowski', 'p': 2},\n",
        "    'manhattan': {'metric': 'minkowski', 'p': 1},\n",
        "    'minkowski_p3': {'metric': 'minkowski', 'p': 3},\n",
        "}\n",
        "\n",
        "print(\"KNN Regressor Performance with Different Distance Metrics:\\n\")\n",
        "for name, params in metrics.items():\n",
        "    knn_reg = KNeighborsRegressor(n_neighbors=5, metric=params['metric'], p=params['p'])\n",
        "    knn_reg.fit(X_train_scaled, y_train)\n",
        "    y_pred = knn_reg.predict(X_test_scaled)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    print(f\"Metric: {name:<15} | Mean Squared Error: {mse:.4f}\")\n",
        "\n",
        "KNN Regressor Performance with Different Distance Metrics:\n",
        "\n",
        "Metric: euclidean       | Mean Squared Error: 0.0304\n",
        "Metric: manhattan       | Mean Squared Error: 0.0304\n",
        "Metric: minkowski_p3    | Mean Squared Error: 0.0356\n",
        "Q43)Train a KNN Classifier and evaluate using ROC-AUC score.\n",
        "Ans:\n",
        "\n",
        "\n",
        "[ ]\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, label_binarize\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# Binarize labels for ROC-AUC computation (One-vs-Rest)\n",
        "y_bin = label_binarize(y, classes=[0, 1, 2])  # shape: (n_samples, n_classes)\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_bin, test_size=0.3, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train the KNN Classifier\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict probability scores\n",
        "y_score = knn.predict_proba(X_test_scaled)\n",
        "\n",
        "# Compute ROC-AUC score (macro-average across classes)\n",
        "roc_auc = roc_auc_score(y_test, y_score, multi_class='ovr', average='macro')\n",
        "\n",
        "print(f\"Multi-class ROC-AUC Score (macro-average): {roc_auc:.4f}\")\n",
        "\n",
        "Q44) Train a PCA model and visualize the variance captured by each principal component.\n",
        "Ans:\n",
        "\n",
        "\n",
        "[ ]\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Fit PCA\n",
        "pca = PCA()\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Explained variance ratio\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "cumulative_variance = explained_variance.cumsum()\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(range(1, len(explained_variance) + 1), explained_variance, alpha=0.6, label='Individual Explained Variance')\n",
        "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', color='r', label='Cumulative Explained Variance')\n",
        "plt.xlabel('Principal Component')\n",
        "plt.ylabel('Explained Variance Ratio')\n",
        "plt.title('PCA - Variance Explained by Each Component')\n",
        "plt.xticks(range(1, len(explained_variance) + 1))\n",
        "plt.legend(loc='best')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "Q45)Train a KNN Classifier and perform feature selection before training.\n",
        "Ans:\n",
        "\n",
        "\n",
        "[ ]\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load Wine dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Feature selection (Select top 8 features based on ANOVA F-test)\n",
        "selector = SelectKBest(score_func=f_classif, k=8)\n",
        "X_train_selected = selector.fit_transform(X_train, y_train)\n",
        "X_test_selected = selector.transform(X_test)\n",
        "\n",
        "# Standardize selected features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_selected)\n",
        "X_test_scaled = scaler.transform(X_test_selected)\n",
        "\n",
        "# Train KNN Classifier\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = knn.predict(X_test_scaled)\n",
        "print(\"Classification Report with Feature Selection:\\n\")\n",
        "print(classification_report(y_test, y_pred, target_names=wine.target_names))\n",
        "\n",
        "Classification Report with Feature Selection:\n",
        "\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "     class_0       0.95      1.00      0.97        19\n",
        "     class_1       1.00      0.90      0.95        21\n",
        "     class_2       0.93      1.00      0.97        14\n",
        "\n",
        "    accuracy                           0.96        54\n",
        "   macro avg       0.96      0.97      0.96        54\n",
        "weighted avg       0.97      0.96      0.96        54\n",
        "\n",
        "Q46) Train a PCA model and visualize the data reconstruction error after reducing dimensions.\n",
        "Ans:\n",
        "\n",
        "\n",
        "[ ]\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load and standardize the Wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Reduce dimensions with PCA\n",
        "n_components = 5  # You can vary this number to observe different errors\n",
        "pca = PCA(n_components=n_components)\n",
        "X_reduced = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Reconstruct data from reduced components\n",
        "X_reconstructed = pca.inverse_transform(X_reduced)\n",
        "\n",
        "# Compute reconstruction error (MSE per sample)\n",
        "reconstruction_errors = np.mean((X_scaled - X_reconstructed) ** 2, axis=1)\n",
        "\n",
        "# Visualization of reconstruction error\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(reconstruction_errors, marker='o', linestyle='-', color='teal')\n",
        "plt.title(f'Reconstruction Error per Sample (PCA with {n_components} Components)')\n",
        "plt.xlabel('Sample Index')\n",
        "plt.ylabel('Mean Squared Error')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Optionally, print average reconstruction error\n",
        "print(f\"Average Reconstruction Error (MSE): {np.mean(reconstruction_errors):.4f}\")\n",
        "\n",
        "\n",
        "Q47) Train a KNN Classifier and visualize the decision boundary.\n",
        "Ans:\n",
        "\n",
        "\n",
        "[ ]\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Load and standardize data\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Reduce to 2D with PCA for visualization\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Train KNN on 2D PCA-transformed data\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_pca, y)\n",
        "\n",
        "# Create a mesh grid for decision boundary visualization\n",
        "x_min, x_max = X_pca[:, 0].min() - 1, X_pca[:, 0].max() + 1\n",
        "y_min, y_max = X_pca[:, 1].min() - 1, X_pca[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 300),\n",
        "                     np.linspace(y_min, y_max, 300))\n",
        "Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "# Plotting the decision boundary and data points\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.Set1)\n",
        "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap=plt.cm.Set1, edgecolor='k', s=40)\n",
        "plt.legend(handles=scatter.legend_elements()[0], labels=wine.target_names)\n",
        "plt.title(\"KNN Decision Boundary (PCA-Reduced Wine Dataset)\")\n",
        "plt.xlabel(\"Principal Component 1\")\n",
        "plt.ylabel(\"Principal Component 2\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "Q48)Train a PCA model and analyze the effect of different numbers of components on data variance.\n",
        "Ans:\n",
        "\n",
        "\n",
        "[ ]\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Load and standardize the Wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train PCA with all components\n",
        "pca = PCA()\n",
        "pca.fit(X_scaled)\n",
        "\n",
        "# Get explained variance ratios\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "cumulative_variance = explained_variance.cumsum()\n",
        "\n",
        "# Plot cumulative explained variance\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', linestyle='--', color='b')\n",
        "plt.axhline(y=0.95, color='r', linestyle='--', label='95% Variance Threshold')\n",
        "plt.title('Cumulative Explained Variance by Number of PCA Components')\n",
        "plt.xlabel('Number of Principal Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.xticks(range(1, len(cumulative_variance) + 1))\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yHej6Nx8en6q"
      }
    }
  ]
}